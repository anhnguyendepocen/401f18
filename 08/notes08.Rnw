%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{8}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
%\newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space
<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
# ANS=FALSE
@

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
# library(broman) # used for myround 
par(mai=c(0.8,0.8,0.1,0.1))
@


\begin{frame}
\frametitle{\CHAPTER. Model diagnostics}

\vspace{-2mm}

\begin{myitemize}
\item
We know how to estimate parameters and make hypothesis tests for linear models.
\item 
We know how to make predictions, with uncertainty estimates, using linear models. 
\begin{enumerate}
\item \enumerateSpace
What if our conclusions depend on our choice of model? 
\item \enumerateSpace
What if our model is a poor description of the data?
\item \enumerateSpace
What if a much better model exists? 
\item \enumerateSpace
What if the model describes some parts of the data okay, but not other parts?
\end{enumerate}
\item How can we answer these questions?
\begin{enumerate}
\item \enumerateSpace
\myemph{Graphical investigations}. Make informative plots.
\item \enumerateSpace
\myemph{Quantitative investigations}. Make relevant statistical tests, or calculate other interpretable statistics.
\end{enumerate}
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Looking for patterns in the residuals}

\begin{myitemize}
\item Recall that the \myemph{residuals} for a linear model are \m{e_1,\dots,e_n} in the linear model \m{\vect{y}=\mat{X}\vect{b}+\vect{e}}.
\item Residuals estimate the measurement errors \m{\epsilon_1,\dots,\epsilon_n} in the probability model \m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}}.
\item \m{\vec{b}} is a noisy estimate of \m{\vec{\beta}}, meaning that we cannot find \m{\vec{\beta}} exactly due to measurement error. Similarly, \m{\vec{e}} is a noisy estimate of \m{\vec{\epsilon}}.
\item The specification that \m{\epsilon_1,\dots,\epsilon_n \sim\iid \; \normal(0,\sigma)} implies the measurement errors have no pattern.
\item Any pattern, or association with some other variable, that we can find in the residuals contradicts the model and could lead to improvements.
\item The search for patterns in the residuals can take creativity and persistence.
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Residuals for time series data}

\begin{myitemize}
\item A fairly common type of data has points collected through time. This type of data is called a \myemph{time series}.
\item For example, the annual data we investigated on  unemployment and life expectancy are both time series.
\item Time series might be expected to have measurements at points close in time that are more similar than those distant in time. If this is true of residuals, the pattern is inconsistent with the iid measurement error model.

\end{myitemize}
\end{frame}

\begin{frame}

\vspace{5mm}

\myquestion. How can we look for temporal patterns in the residuals? Think of (at least) two plots to make.

\answer{\vspace{100mm}}{(i) A timeplot plots residuals against time; (ii) plotting $e_i $ against $e_{i-1}$ for $i=2,\dots,n$ looks for correlations between neighboring residuals.}

\end{frame}

<<reconstruct_variables,echo=F>>=
L <- read.table(file="life_expectancy.txt",header=TRUE)
L_fit <- lm(Total~Year,data=L)
L_detrended <- L_fit$residuals
U <- read.table(file="unemployment.csv",sep=",",header=TRUE)
U_annual <- apply(U[,2:13],1,mean)
U_detrended <- lm(U_annual~U$Year)$residuals
L_detrended <- subset(L_detrended,L$Year %in% U$Year)
@

\begin{frame}[fragile]
\frametitle{Residuals for unemployment vs life expectancy}

\vspace{-3mm}

\begin{myitemize}
\item Recall the linear model relating life expectancy to unemployment:
<<lm1>>=
lm1 <- lm(L_detrended~U_detrended)
@
\item Some graphical investigations of the residuals follow on the next slide.
\item One way to see if the residuals have statistically noticeable dependence is to fit a linear model to the residuals \m{e_{1:n}} of the form
\mydisplaymath{
e_i = b e_{i-1} + h_i,\quad \mbox{for } i=2,3,\dots,n,
}
where \m{h_i} is the residual error when \m{e_{i-1}} is used to predict \m{e_i}.
\end{myitemize}
\myquestion. Why do we not need an intercept here?

\answer{\vspace{30mm}}{The residuals are centered on zero by their construction, as long as there is an intercept in the model.}


\end{frame}

\begin{frame}

\vspace{5mm}

\myquestion. How would you fit the linear model 
\mydisplaymath{
e_i = b e_{i-1} + h_i,\quad \mbox{for } i=2,3,\dots,n,
}
for the residuals from \code{lm1 <- lm(L_detrended~U_detrended)} in R?


\answer{\vspace{50mm}}{
\code{n <- length(L_detrended)} \\
\code{e <- resid(lm1)[2:n]} \\
\code{lag_e <- resid(lm1)[1:(n-1)]}\\
\code{lm(e~lag_e-1)}
}

\end{frame}


\begin{frame}[fragile]
\frametitle{}
<<lag_lm>>=
n <- length(resid(lm1))
e <- resid(lm1)[2:n]
lag_e <- resid(lm1)[1:(n-1)] # NOTE WE NEED 1:(n-1) NOT 1:n-1
lm2 <- lm(e~lag_e-1)
head(model.matrix(lm2),3)
summary(lm2)$coef
@

\vspace{-2mm}

\myquestion. What do you conclude from this analysis?

\answer{\vspace{20mm}}{The residuals are much more highly correlated than can be explained by chance variation.}

\end{frame}

%\end{document}

\begin{frame}[fragile]


\begin{columns}[T] 
\begin{column}{0.45\textwidth}
<<timeplot_code,eval=F,echo=T>>=
plot(U$Year,resid(lm1))
@

\vspace{-12mm}

<<timeplot_plot,fig.width=3,fig.height=3.5,out.width="2in",echo=F>>=
<<timeplot_code>>
@
\end{column}
\begin{column}{0.45\textwidth}
<<lagplot_code,eval=F,echo=T>>=
plot(lag_e,e)
@

\vspace{-12mm}

<<lagplot_plot,fig.width=3,fig.height=3.5,out.width="2in",echo=F>>=
<<lagplot_code>>
@
\end{column}
\end{columns}


\vspace{-3mm}

\myquestion. What do you these plots tell you about (i) the least squares estimate of the association between changes of life expectancy and unemployment; (ii) its standard error and confidence interval?

\answer{\vspace{20mm}}{The model assumption of independent measurement errors is far from the reality. All standard errors and confidence intervals built on this assumption are therefore brought into question.}

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Why do the detrended residuals have a trend?}

\vspace{-5mm}

\begin{myitemize}
\item Recall the code we used to construct \code{L_detrended} and \code{U_detrended}
<<construction,echo=T,eval=F>>=
<<reconstruct_variables>>
@
\end{myitemize}

\vspace{-1.5mm}

\myquestion. We removed a linear trend from both life expectancy and unemployment. What does that mean? What is the equation for the model that we have fitted?

\answer{\vspace{30mm}}{It means we fitted a sample model
\[
y_{i}= a+bi+e_i, \quad i=1,2,\dots,n
\]
with $y_i$ being either the life expectancy or unemployment for the $i$th year in the dataset. The detrended data are the residuals $e_i$, $i=1,\dots,n$ from this regression.
}

\end{frame}


%\end{document}

\begin{frame}[fragile]

\vspace{5mm}

\myquestion. It is a statistical detective puzzle to figure out how the residuals from \code{lm1} can have a linear trend when all the variables are detrended. Any ideas? Plotting the variables may give a clue.

\answer{\vspace{5mm}}{Look at the axes. Careful reading of the code reveals (after plenty of head-scratching) that the detrending was done before subsetting \code{L_detrended} to make the times match \code{U_detrended}. The removed part was trending up and the remaining part trends down.}

\end{frame}

%\end{document}

\begin{frame}[fragile]
<<plot_L_code,eval=F,echo=T>>=
plot(Total~Year,data=L,type="l") # L is life expectancy
@

\vspace{-18mm}

<<plot_L,echo=F,fig.height=3,fig.width=6,out.width="4.5in">>=
<<plot_L_code>>
@

\vspace{-3mm}

<<plot_U_code,eval=F,echo=T>>=
plot(U_annual~Year,data=U,type="l") # U is unemployment
@

\vspace{-18mm}

<<plot_U,echo=F,fig.height=3,fig.width=6,out.width="4.5in">>=
<<plot_U_code>>
@

\end{frame}



\begin{frame}[fragile]
<<plot_L_detrended_code,eval=F,echo=T>>=
L_fit <- lm(Total~Year,data=L)
L_detrended <- L_fit$residuals
plot(L_detrended~Year,data=L)
@

\vspace{-16mm}

<<plot_L_detrended,echo=F,fig.height=3,fig.width=5,out.width="3.3in">>=
<<plot_L_detrended_code>>
@

\vspace{-5mm}

<<plot_L_detrended_subset_code,eval=F,echo=T>>=
L_detrended <- subset(L_detrended,L$Year %in% U$Year)
plot(L_detrended~Year,data=U)
@

\vspace{-16mm}

<<plot_L_detrended_subset,echo=F,fig.height=3,fig.width=5,out.width="3.3in">>=
<<plot_L_detrended_subset_code>>
@

\end{frame}




\begin{frame}[fragile]
\frametitle{Rescuing the life expectancy/unemployment analysis}

\vspace{-4mm}

\begin{myitemize}
\item We have found a serious problem with our linear model analysis. 
\item From a statistically significant coefficient, we inferred counter-intuitively that higher unemployment is associated with above-trend life expectancy.
\item \myemph{A p-value is only as good as the probability model producing it.} 
\item We have found that the probability model we used is seriously defective. It is based on assumptions that are substantially violated.
\item This doesn't necessarily mean that the result is right or wrong. 
\item It means we haven't yet found a good argument either way.
\item This topic is of current interest:
\url{https://www.nytimes.com/2017/10/16/upshot/how-a-healthy-economy-can-shorten-life-spans.html}
\end{myitemize}

\vspace{-1mm}

\myquestion. Can we do a better data analysis? How?

\answer{
\vspace{15mm}
}{We will find out one way later.}
\end{frame}


\begin{frame}[fragile]
\frametitle{Outliers}

\begin{myitemize}
\item Sometimes one, or a few, points are inconsistent with a model that explains the rest of the data nicely.
\item These points are called \myemph{outliers}.
\item Our first responsibility is to notice them.
\item Our second responsibility is to work out whether they affect the conclusions of the analysis. If they don't, the issue becomes unimportant.
\end{myitemize}
\myquestion. It is tempting to remove clear outliers from the data analysis on the assumption that they are errors. When is that reasonable? When is it a bad decision?

\answer{\vspace{30mm}}{If you have good reason to believe they are errors, and you explain clearly in your analysis what you did, that is okay. However, outliers can be the most informative points: they can tell you about some unexpected special situation. Unexpected data points can be scientific discoveries: Alexander Fleming noticed an accidental mold contamination killed his cultured acteria.}

\end{frame}


\begin{frame}[fragile]
\frametitle{Outliers and responsible scientific conduct}

\begin{myitemize}
\item \myemph{Falsification} is the manipulation of research materials, equipment, or processes or changing or omitting data or results such that the research is not accurately represented in the research record (\url{https://en.wikipedia.org/wiki/Scientific_misconduct}).
\end{myitemize}
\myquestion. How could inappropriate treatment of outliers lead to charges of falsification? What can a careful data analyst do to avoid that?

\answer{\vspace{40mm}}{Suppose you remove outliers that do not match your hypothesis, believing them to be errors. If you don't acknowledge or document your action, you might be accidentally suppressing important information, manipulating the scientific record to make it look like it agrees with your hypothesis.}


\end{frame}


\begin{frame}[fragile]
\frametitle{Leverage and influence}

\begin{myitemize}
\item A data point has high \myemph{leverage} if its explanatory variables are distant from much of the rest of the data, so the point plays a relatively large role in determining the fitted values.
\item Leverage of a point \m{i} depends only on the design matrix \m{\mat{X}=[x_{ij}]_{n\times p}}, and primarily on  \m{x_{i1},\dots,x_{ip}}.
\item A point has high \myemph{influence} if removing that point leads to large changes in the parameter estimates and fitted values.
\item Influence depends on both \m{\mat{X}} and \m{\vect{y}}.
\item An outlier with high leverage is a point of very high influence. 
\end{myitemize}

\end{frame}


\begin{frame}[fragile]

\vspace{5mm}

\myquestion. Sketch a scatterplot (i.e., a plot of \m{\vect{y}} against a single explanatory vector \m{\vect{x}}) that has a point of high leverage, but not high influence.

\answer{\vspace{50mm}}{A cluster of x-values and an outlier in the x direction which is close to the linear regression line in the y direction.
Archimedes: ``Give me a lever and a place to stand and I will move the earth.''}

\end{frame}

\begin{frame}[fragile]

\vspace{5mm}

\myquestion. Sketch a scatterplot that has a point of high leverage which is also a point of high influence.

\answer{\vspace{50mm}}{A cluster of x-values and an outlier in the x direction which is also an outlier from the linear regression line in the y direction.}

\end{frame}

\begin{frame}[fragile]

\vspace{5mm}

\myquestion. Sketch a scatterplot that has an outlier which is not influential.

\answer{\vspace{50mm}}{An outlier in the middle of a simple linear regression  scatterplot.}

\end{frame}

\begin{frame}[fragile]
\frametitle{Practical strategies for influence and leverage}

\begin{myitemize}
\item A small collection of points with unusual and extreme values of the explanatory variables will likely have high leverage and may also have high influence.
\item Try removing these points to see if that changes the conclusions of your data analysis. If it does, then hard thought is required.
\item A measure of influence is \myemph{Cook's distance}, which is computed for a model \code{lm1} by \code{cooks.distance(lm1)}.
\item We are not going to study Cook's distance carefully. You can investigate the points which have the highest Cook's distance. For example, you can see the effect of removing these points on your conclusions.

\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Normality}

\begin{myitemize}
\item If the number of points is fairly large (say, more than 30) the estimates of the coefficients in the linear model have a \myemph{central limit theorem}.
\item Recall that a basic central limit theorem says that the average of many independent identically distributed (iid) random variables approximately follows a normal distribution.
\item The least squares estimates of coefficients can be thought of as a kind of averaging of the data. This argument suggests (correctly!) that the distribution of these estimates should also follows a central limit theorem.
\item Measurement error with very long tails may lead to observations that look like  outliers. They may also behave like outliers, and potentially have high influence.
\item Usually, because of the central limit theorem, normality of errors is not especially important. It is more important for prediction intervals.
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Non-constant variance}

\begin{myitemize}
\item Our usual probability model assumes (in addition to normality and independence) that the measurement errors have constant variance.
\item Plotting the residuals (say, against fitted values or against time or against some other variable) may show that the spread of the residuals is larger in some places than others.
\item Taking the logarithm of non-negative data help surprisingly often in this case.
\item Other approaches to deal with this problem are beyond this course, though you now have the necessary background to learn and use these methods. These involve models with a different measurement model from the constant variance model \m{\epsilon_1,\dots,\epsilon_n\sim \iid\;\normal(0,\sigma)}.

\end{myitemize}

\end{frame}


\end{document} %%%%%%%%%%%%%%%%%%%%%%%%%%

------- This is just for copying to make new slides ---------

\end{frame}

\begin{frame}[fragile]
\frametitle{}

\begin{myitemize}
\item 
\end{myitemize}

\end{frame}
