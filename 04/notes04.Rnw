%\documentclass[handout]{beamer}
\documentclass{beamer}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space
<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\input{../header.tex}
\newcommand\CHAPTER{4}
%\newcommand\LSi{\mathrm{(LS1)}}
%\newcommand\LSii{\mathrm{(LS2)}}
%\newcounter{tXX}
%\newcounter{tXy}
%\newcounter{matrixLSi}

\begin{document}

% knitr set up
<<knitr_opts,echo=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE, 
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)

options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
library(broman) # used for myround 
@


\begin{frame}[fragile]
\frametitle{Chapter \CHAPTER. Probability models}

\vspace{-1mm}

\begin{myitemize}
\item A \myemph{probability model} is an assignment of probabilities to possible outcomes. 

\item We don't observe these probabilities. We observe a particular dataset.

\item If we treat the dataset as an outcome of a probability model, we can answer questions such as,

\vspace{3mm}

``{\it If there really is no association between unemployment and life expectancy, what is the probability we would see a least squares linear model coefficient as large as the one we actually observed, due to random fluctuations in the data?}''

\vspace{3mm}

\item Here, we are particularly interested in developing a probability model for the linear model.

\item First, we need some basic tools for probability models: random variables, the normal distribution, mean, variance and standard deviation.

\end{myitemize}
\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Random variables and events}

\begin{myitemize}
\item A \myemph{random variable} \m{X} is a random number with probabilities assigned to outcomes.

Example:  Let \m{X} be a roll of a fair die. A natural probability model is to assign probability of \m{1/6} to each of the possible outcomes \m{1,2,3,4,5,6}.

\item An \myemph{event} is a set of possible outcomes.

Example:  For a die, \m{E=\{X\ge 4\}=\{4,5,6\}} is the event that the die shows \m{4} or more.

\item We can assign probabilities to events just like to outcomes.

Example: For a die, \m{\prob(E)=\prob(X\ge 4)= 3/6 = 1/2}.

\end{myitemize}
\myquestion. If an experiment can be repeated many times (like rolling a die) how can you check whether the probability model is correct?

\answer{\vspace{20mm}}{For a correct probability model, the probability of an event should match the long run proportion of the time that the event happens in many repetitions. This is the \myemph{law of large numbers}}. 

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Notation for combining events}
\begin{myitemize}
\item \m{\{\mbox{$E$ or $F$}\}} is the event that either \m{E} or \m{F} or both happens.
\item Since \m{E} and \m{F} are sets, we can write this as a union,
 \m{\{\mbox{$E$ or $F$}\}= E\cup F}
\item \m{\{\mbox{$E$ and $F$}\}} is the event that both \m{E} and \m{F} happen.
\item We can write this as an intersection,
\mydisplaymath{
  \{\mbox{$E$ and $F$}\}=E\cap F
}
\item Usually, we prefer ``and/or'' to  ``intersection/union''. 
\end{myitemize}
\myquestion. When does this formal use of ``and'' and ``or'' agree with usual English usage? When does it disagree?

\answer{\vspace{20mm}}

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{The basic rules of probability}

\vspace{-1.5mm}

\begin{enumerate}
\item Probabilities are numbers between 0 (impossible) and 1 (certain).

\item  Let \m{\Sspace} be the set of all possible outcomes. Then, \m{\prob(\Sspace)=1}.

Example: For a die, \m{\prob(X\in\{1,2,3,4,5,6\})=1}.

\item Events \m{E} and \m{F} are called \myemph{mutually exclusive} if they cannot happen at the same time. In other words, their intersection is the empty set. In this case,

\vspace{-4mm}

\mydisplaymath{
\prob(\mbox{$E$ or $F$})=\prob(E)+\prob(F).
}

%\end{myitemize}
\end{enumerate}

\myquestion. You roll a red die and a blue die. Let \m{E=\{\mbox{red die shows 1}\}}, \m{F=\{\mbox{blue die shows 1}\}}, \m{G=\{\mbox{red die shows 6}\}}. (a) Are \m{E} and \m{F} mutually exclusive? (b) How about \m{E} and \m{G}? (c) How about \m{F} and \m{G}?

\answer{\vspace{25mm}}{TODO}

\end{frame}


%\end{document}

\begin{frame}[fragile]
\frametitle{Discrete random variables}
\begin{myitemize}
\item \m{X} is a  \myemph{discrete random variable} if we can list all its possible outcomes. Let's call them \m{x_1,x_2,\dots}. 

\item A discrete random variable is specified by probability that the random variable takes each possible outcome,
\mydisplaymath{
  p_i = \prob[X=x_i], \mbox{for $i = 1,2,3,\dots$}
}

\item It can be helpful to plot a graph of \m{p_i} against \m{x_i}.
\item This graph is called the \myemph{probability mass function}.
\end{myitemize}
\myquestion. Sketch the probability mass function for a fair die.

\answer{\vspace{40mm}}

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Simulating the law of large numbers}
\begin{myitemize}
\item The ``law of large numbers'' says that the proportion of each outcome \m{i} in a large number of draws of a discrete random variable approaches \m{p_i}.

\item We can test this by simulation, using the \code{replicate()} command.
\end{myitemize}

\WorkedExample. In R, a random draw with replacement from \m{\{1,2,3,4,5,6\}} can be obtained by
\code{sample(1:6,size=1)}
This is equivalent to one roll of a fair die.

%\vspace{-5mm}

<<hist100,echo=T,eval=F,purl=TRUE>>=
hist(replicate(n=100,sample(1:6,size=1) ),
  main="",prob=TRUE,breaks=0.5:6.5,xlab="n=100",ylim=c(0,0.21))
@

<<hist10000,eval=F,echo=F,purl=TRUE>>=
hist(replicate(sample(1:6,size=1),n=10000),
  main="", prob=TRUE, breaks=0.5:6.5, xlab="n=10000",ylim=c(0,0.21))
@

\vspace{-15mm}

\begin{columns}[T] 
\begin{column}{0.45\textwidth}
<<hist100eval,out.width="60mm",fig.width=3.5,fig.height=3,echo=F,purl=F>>=
set.seed(23)
<<hist100>> 
@
\end{column}


\begin{column}{0.45\textwidth}

<<hist10000eval,out.width="60mm",fig.width=3.5,fig.height=3,echo=F,purl=F>>=
set.seed(196)
<<hist10000>> 
@
\end{column}
\end{columns}

%hist(replicate(sample(1:6),n=10),
%histogram(sample(1:6,n=100))
%histogram(sample(1:6,n=1000))
%histogram(sample(1:6,n=10000))

\end{frame}


%\end{document}
\begin{frame}[fragile]
\frametitle{Continuous random variables: the normal distribution}

\vspace{-1mm}

\begin{myitemize}
\item 
A \myemph{continuous random variable} is one which can take any value in an interval of the real numbers.

Example: physical quantities such as time and speed are not limited to a discrete set of possible values.

\item We will often see the \myemph{normal distribution}.

\item Let's look at \myemph{normal random variables} simulated by R using \code{rnorm()}.
\end{myitemize}

\vspace{-1mm}

<<rnorm_seed,echo=F,purl=F>>=
set.seed(928)
@

<<rnorm_example,echo=T>>=
rnorm(n=10,mean=20,sd=5)
@

\begin{myitemize}
\item The arguments \code{mean=20, sd=5} of \code{rnorm()} are the \myemph{parameters} of the normal distribution.
\item A normal random variable can take any numeric value:  it is continuous.
\item Values are centered on the mean and are usually less than twice the standard deviation (sd) from the mean.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{A histogram of normal distribution simulations}

<<rnorm100,echo=T,eval=F,purl=TRUE>>=
hist(rnorm(n=100,mean=20,sd=5),
  main="",xlab="n=100")
@

<<rnorm10000,eval=F,echo=F,purl=TRUE>>=
hist(rnorm(n=10000,mean=20,sd=5),
  main="",xlab="n=10000")
@

\vspace{-15mm}

\begin{columns}[T] 
\begin{column}{0.45\textwidth}
<<rnorm100eval,out.width="60mm",fig.width=3.5,fig.height=3,echo=F,purl=F>>=
set.seed(676)
<<rnorm100>> 
@
\end{column}


\begin{column}{0.45\textwidth}

<<rnorm10000eval,out.width="60mm",fig.width=3.5,fig.height=3,echo=F,purl=F>>=
set.seed(117)
<<rnorm10000>> 
@
\end{column}
\end{columns}

\begin{myitemize}
\item Large samples from the normal distribution follow a \myemph{bell curve} histogram.
\item From smaller samples, this is harder to see.
\end{myitemize}

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Finding probabilities for a continuous random variable}
\begin{myitemize}
\item A continuous random variable \m{X} has a \myemph{probability density function} \m{f(x)} which is integrated to find the probability that \m{X} falls in any interval:

\vspace{-3mm}

\mydisplaymath{
\prob(a < X < b) = \int_a^b f(x)\, dx
}
\item Write \m{X\sim\normal(\mu,\sigma)} to mean \m{X} is a normal random variable with mean \m{\mu} and sd \m{\sigma}. The probability density function of \m{X} is
\mydisplaymath{
f(x) =\frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2/2\sigma^2}
}

\vspace{-2mm}

and so
\mydisplaymath{
\prob(a < X < b) = \int_a^b
\frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2/2\sigma^2}\, dx
}
\item  This integral has no closed form solution.
\item Fortunately, R provides \code{pnorm()} and \code{qnorm()} that let us work with probabilities for the normal distribution numerically.

\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Calculating probabilities for the normal distribution}
\begin{myitemize}
\item \code{pnorm()} finds the \myemph{left tail} of the normal distribution.


\vspace{-10mm}

<<normal_pdf,out.width="60mm",fig.width=3.5,fig.height=3,echo=F,purl=T>>=
# Create data for the area to shade
pnorm_plot <- function(q,mean,sd,...) {
  coord.x <- seq(from=mean-3*sd,to=mean+3*sd,by=sd/100)
  plot(coord.x,dnorm(coord.x,mean,sd),type="l",...)
  shaded.coord.x <- coord.x[coord.x <= q]
  polygon.x <- c(mean-3*sd,shaded.coord.x,q)
  polygon.y <- c(0,dnorm(shaded.coord.x,mean,sd),0)  
  polygon(polygon.x,polygon.y,col='skyblue')
}
pnorm_plot(25,20,5,main="",xlab="",ylab="probability density")
@

\vspace{-10mm}

Example: \code{pnorm(25,mean=20,sd=5)} computes the shaded area above.

\item We don't have to do calculus with the normal integral, but we do use the relationship between the curve, area under the curve, and probability. And we must know how to compute these things in R.

\item For \m{X\sim\normal(\mu,\sigma)},

\vspace{-3mm}

\mydisplaymath{
\mbox{\code{pnorm(x,mu,sigma)}} =\prob(X\le x) = \int_{-\infty}^x
\frac{1}{\sqrt{2\pi\sigma^2}} e^{-(y-\mu)^2/2\sigma^2}\, dy
}

\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Finding probabilities that are not a left tail}

\myquestion. Let \m{X\sim \normal(10,10)}. Sketch a shaded area under a curve giving \m{\prob(0\le X\le 10)}. Write this probability as an integral and as R code.

\vspace{1.5mm}

Hint: To use \code{pnorm()}, think of the shaded area as the difference of two left tails.

\answer{\vspace{80mm}}{TODO}

\end{frame}

\begin{frame}[fragile]
\frametitle{Other continuous distributions}
\begin{myitemize}
\item There are many continuous random variables that are not normally distributed.

\item For example, we could explore the \myemph{uniform} or \myemph{exponential} distributions.


\vspace{1mm}

%\vspace{-5mm}

\begin{columns}[T] 
\begin{column}{0.45\textwidth}
<<runif,echo=T,eval=F,purl=TRUE>>=
hist(runif(100))
@

\vspace{-8mm}

<<runif_eval,out.width="60mm",fig.width=3.5,fig.height=3,echo=F,purl=F>>=
set.seed(28)
hist(runif(100),xlab="",main="")
@
\end{column}


\begin{column}{0.45\textwidth}

<<rexp,eval=F,echo=T,purl=TRUE>>=
hist(rexp(100))
@

\vspace{-8mm}

<<rexp_eval,out.width="60mm",fig.width=3.5,fig.height=3,echo=F,purl=F>>=
set.seed(31)
hist(rexp(100),xlab="",main="")
@
\end{column}
\end{columns}
<<,eval=F>>=
@

\vspace{-8mm}

\item Normal random variables are the building block for all the random variables we work with in this class.
\item Let's investigate why the normal distribution is so important.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Sums and averages follow the normal distribution}


\vspace{-1.5mm}

\begin{myitemize}
\item When we sum many random quantities, the sum often follows a normal distribution even if each term in the sum is not normally distributed.

\item This property is called \myemph{the central limit theorem}.

\item It is an empirical fact as well as a mathematical theorem!

\item Averaging is multiplying the sum by a constant (\m{1/n}). A bell curve is still a bell curve when we rescale by multiplication.

\end{myitemize}
\myquestion. Would you expect a histogram of student heights to follow a normal curve? Why? Why  not?

\answer{\vspace{15mm}}{TODO}

\myquestion. Would you expect a histogram of the daily change in the Dow Jones stock market index to follow a normal curve? Why? Why not?

\answer{\vspace{15mm}}{TODO}

\end{frame}

\begin{frame}[fragile]
\frametitle{Demonstrating the central limit theorem with dice}

<<,purl=F,echo=F>>=
set.seed(199)
@

\begin{myitemize}
\item \code{sample(1:6,2,replace=TRUE)} simulates the outcome of rolling two dice.
\item \code{sum(sample(1:6,2,replace=TRUE))} simulates the sum of two dice.
\item \code{replicate()} lets us see what happens if we do this many times
\end{myitemize}
<<dice_rolls>>=
dice2 <- replicate(50000,sum(sample(1:6,2,replace=TRUE)))
dice3 <- replicate(50000,sum(sample(1:6,3,replace=TRUE)))
dice10 <- replicate(50000,sum(sample(1:6,10,replace=TRUE)))
dice20 <- replicate(50000,sum(sample(1:6,20,replace=TRUE))) 
@
\begin{myitemize}
\item Guess how many dice you have to add up before the histogram looks normal?
\end{myitemize}
\end{frame}

\begin{frame}[fragile]

\vspace{-5mm}

<<two_dice,echo=T,eval=F,purl=T>>=
hist(dice2,prob=TRUE,breaks=(min(dice2)-0.5):(max(dice2)+0.5))
normal.x <- seq(from=min(dice2),to=max(dice2),length=100)
normal.y <- dnorm(normal.x,mean=mean(dice2),sd=sd(dice2)) 
lines(normal.x,normal.y,col="blue")
@


<<,echo=F,purl=T>>=
dice_hist <- function(dat,...){
  hist(dat,probability=TRUE,breaks=seq(min(dat)-0.5,max(dat)+0.5,by=1),...)
  normal.x <- seq(from=min(dat),to=max(dat),length=100)
  normal.y <- dnorm(normal.x,mean=mean(dat),sd=sd(dat)) 
  lines(normal.x,normal.y,col="blue")
}
@

\vspace{-5mm}

\begin{columns}[T] 
\begin{column}{0.45\textwidth}
\centerline{2 dice}

\vspace{-10mm}

<<dice2,echo=F,eval=T,purl=F,out.width="50mm",fig.width=3.5,fig.height=3>>=
dice_hist(dice2,main="",xlab="")
@

\vspace{-9mm}

\centerline{10 dice}

\vspace{-11mm}

<<dice10,echo=F,eval=T,purl=F,out.width="50mm",fig.width=3.5,fig.height=3>>=
dice_hist(dice10,main="",xlab="",by=1)  
@

\end{column}


\begin{column}{0.45\textwidth}

\centerline{3 dice}

\vspace{-10mm}

<<dice3,echo=F,eval=T,purl=F,out.width="50mm",fig.width=3.5,fig.height=3>>=
dice_hist(dice3,main="",xlab="",ylim=c(0,0.14)) 
@

\vspace{-9mm}

\centerline{20 dice}

\vspace{-11mm}

<<dice20,echo=F,eval=T,purl=F,out.width="50mm",fig.width=3.5,fig.height=3>>=
dice_hist(dice20,main="",xlab="") 
@

\end{column}
\end{columns}

\vspace{-8mm}

\myquestion. Why do we use \code{prob} and \code{breaks} arguments to \code{hist()}?

\end{frame}

\begin{frame}
\frametitle{More normal approximation situations}

\myquestion. Would you expect detrended data on life expectancy at birth to follow a normal distribution? Explain.

\answer{\vspace{20mm}}{TODO}

\myquestion. Consider the mice weight data for HW1 with mice randomized to two treatments: a high fat diet and a usual lab diet. (a) Would you expect the weights of mice in each treatment group to follow a normal distribution? (b) If the experiment were replicated ten times, and an average weight calculated for each of these ten replications, would you expect the ten averages to follow a normal distribution? Are your answers different for (a) and (b)?

\answer{\vspace{20mm}}{TODO}

\end{frame}

%\end{document}


\begin{frame}[fragile]
\frametitle{The sample mean and the expectation of a random variable}

\begin{myitemize}
\item The \myemph{sample mean} or \myemph{average} of \m{\vect{y}=(y_1,\dots,y_n)} is
\mydisplaymath{
 \bar{\vect{y}} = \frac{1}{n} \sum_{i=1}^n y_i
}
\item The \myemph{expected value} of a random variable \m{X} taking possible values \m{x_1,x_2,\dots} with probabilities \m{p_1,p_2,\dots} is
\mydisplaymath{
 \E[X] = \sum_{i=1}^\infty x_i p_i
}
\item If we have many draws of \m{X}, the sample proportion taking value \m{x_i} becomes close to \m{p_i} and so the sample mean becomes close to the expected value.
\item If \m{X} is a continuous random variable with density \m{f(x)} the sum for expected value becomes an integral,
\mydisplaymath{
 \E[X] = \int_{-\infty}^\infty x f(x)\, dx
}
\end{myitemize}
\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{The sample mean and expectation using R}

\begin{myitemize}
\item We've already seen the \code{mean()} function which computes the sample mean of a numeric vector.
\item One way to compute the expected value of a random variable is to take the sample mean of many realizations of the random variable.
<<,echo=F>>=
set.seed(1)
@
<<>>=
x <- rnorm(10000,mean=20,sd=5)
mean(x)
@
\item The same calculation can be done using \code{replicate()}:
<<,echo=F>>=
set.seed(1)
@
<<>>=
y <- replicate(n=10000,rnorm(1,mean=20,sd=5))
mean(y)
@
\item We can guess (correctly!) that the expected value of a normal random variable matches its \code{mean} parameter.
\item The expected value of a random variable is sometimes called its mean. We prefer "expected value'' to distinguish from the ``sample mean.''

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{The expected value is not necessarily a possible value}
\myquestion. Compute the expected value of a roll of a fair six-sided die.

(a) By using the definition \m{\E[X] = \sum_{i=1}^\infty x_i p_i}.

\answer{\vspace{30mm}}{TODO}

(b) By averaging a large number of simulated dice using R. Write some R code that is a starting point for testing and debugging.

\answer{\vspace{30mm}}

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{The sample variance and the variance of a random variable}

\begin{myitemize}
\item The \myemph{sample variance} of \m{\vect{y}=(y_1,\dots,y_n)} is
\mydisplaymath{
 \varSample(\vect{y}) = \frac{1}{n-1} \sum_{i=1}^n (y_i-\bar{\vect{y}})^2
}
\item The \myemph{variance} of a random variable \m{X} is defined in terms of the expected value as
\mydisplaymath{
 \var(X) = \E\Big[ \big(\,X-\E[X]\,\big)^2\Big]
}
\item If \m{X} is a random variable, then so is \m{Y=\big(\,X-\E[X]\,\big)^2}. Each possible outcome of \m{X} (say, \m{X=x}) matches an outcome \m{Y=\big(\,x-\E[X]\,\big)^2}.
\item Collections of numbers have a {\it sample variance} computed by \m{\varSample} (not capitalized). Random variables have a {\it variance} computed with \m{\var} (capitalized). 
\item People do not always make this distinction, but we will try to.
\item In R, \code{var()} calculates the sample variance.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Standard deviation}

\begin{myitemize}
\item The \myemph{sample standard deviation} of \m{\vec{y}=(y_1,\dots,y_n)} is the square root of the sample variance.
\mydisplaymath{
\sdSample(\vect{y})=\sqrt{\varSample(\vect{y})}
}
\item The \myemph{standard deviation} of a random variable \m{X} is the square root of its variance.
\mydisplaymath{
\sd(X)=\sqrt{\var(X)}
}
\item In R, \code{sd()} computes the sample standard deviation.
\item We can compute \m{\sd(X)} as the sample standard deviation of a large number of draws of the random variable \m{X}.
<<,echo=F>>=
set.seed(1)
@
<<>>=
x <- rnorm(10000,mean=20,sd=5)
sd(x)
@
\item As we might anticipate, this confirms that the \code{sd} parameter of the normal distribution matches its standard deviation.
\end{myitemize}
\end{frame}

\end{document}


