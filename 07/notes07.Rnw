%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{7}
%\newcounter{CovSum}
%\newcounter{CovSumII}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space
<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
# ANS=FALSE
@

\begin{document} 

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
# library(broman) # used for myround 
@


\begin{frame}
\frametitle{Chapter~\CHAPTER. Factors and the ANOVA F-test}

\begin{myitemize}
\item A \myemph{factor} is an explanatory variable with discrete levels.

\item Factors are also called \myemph{categorical variables}.

\item The different values the variable can take are called \myemph{levels} of the factor.

\item   If we tested growth of a plant in three different soil types we might model this using a soil type factor with 3 levels, \code{clay}, \code{sand} and \code{loam}.

\item A factor with 2 levels is a \myemph{binary factor}.

\item In linear models, factors can describe different classes of units. For example, in HW7, the binary factor \code{Competition} distinguishes certain types of newspaper.

\item We could have a different mean and/or different slope for each level of the factor.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Comparing two sample means via a model with a factor}
\begin{myitemize}
\item Recall the mouse weight experiment. 24 mice are randomized to one of two diets and are weighed after two weeks.
\item First, set up notation. Let \m{y_{ij}} be the weight of the \m{j}th mouse on treatment \m{i}, where \m{i=1,2} corresponds to the normal and high fat diet respectively and \m{j=1,\dots,12} enumerates the replicates for each treatment group.
\item A probability model for this experiment is
\mydisplaymath{
Y_{ij}= \mu_i + \epsilon_{ij} \quad \mbox{ for $i=1,2$ and $j=1,\dots,12$} 
}
where \m{\epsilon_{ij}\sim\iid\;\normal(0,\sigma)}.
\item Here, we have written the model in \myemph{double subscript form}. We have a mean for each level of the treatment group factor.
\item This looks superficially different from the way we have written linear models. There is an extra subscript. 
\item We can rewrite it to make it fit into our linear model framework by putting all the \m{(i,j)} values in a single column.
\end{myitemize}


\end{frame}

%\end{document}


\begin{frame}[fragile]
\frametitle{Dummy variables to code levels of factors}

\begin{myitemize}
\item Let \m{\vec{x}_1=(x_{1,1},\dots,x_{24,1})=(1,\dots,1,0,\dots,0)} be a vector with 1 in the first 12 places and 0 in the remaining 12 places.
\item Let \m{\vec{x}_2=(x_{1,2},\dots,x_{24,2})=(0,\dots,0,1,\dots,1)} be a vector with 0 in the first 12 places and 1 in the remaining 12 places.
\item Let \m{\vec{y}=(y_1,\dots,y_{24})=(y_{1,1},\dots,y_{1,12},y_{2,1},\dots,y_{2,12})} be the mouse weights concatenated into a single vector.
\item Let \m{\vec{e}=(e_1,\dots,e_{24})=(e_{1,1},\dots,e_{1,12},e_{2,1},\dots,e_{2,12})} be residual error terms concatenated into a single vector.
\end{myitemize}
\myquestion. \m{\vec{x}_1} and \m{\vec{x}_1} are called \myemph{dummy variables} since they are built to allow us to write
\m{
  y_{ij}=\mu_i + e_{ij}
}
in the usual single-subscript linear model form,
\mydisplaymath{
  y_k = \mu_1 x_{k,1} + \mu_2 x_{k,2} + e_k.
}
Convince yourself that these equations are equivalent.

\answer{\vspace{20mm}}{$y_k$ has fitted value $\mu_1$ when $k$ corresponds to a mouse fed normal chow, and $\mu_2$ otherwise. This matches the double subscript notation.}
 

\end{frame}

\begin{frame}[fragile]
\frametitle{Two things to notice about models with factors}

We consider the sample linear model \m{y_k = \mu_1 x_{k,1} + \mu_2 x_{k,2} + e_k}, \m{k=1,\dots,24}.

\vspace{2mm} 

\myquestion. Usually we use \m{i} as a subscript when writing a linear model in subscript form. Why do we use \m{k} here?

\answer{\vspace{25mm}}{We have used $i$ and $j$ for the double subscript form. It is best to use a different letter here, for this different context. Formally, the choice of letter doesn't matter.}

\myquestion. Notice there is no intercept term in this linear model. Why?

\answer{\vspace{30mm}}{There are two different means in this model, one for each treatment group. Therefore, we only need two parameters, $\mu_1$ and $\mu_2$. A third intercept parameter is unnecessary.}

\end{frame}

%\end{document}

\begin{frame}[fragile]


\vspace{5mm}

\myquestion. Write the probability model \m{Y_{ij}= \mu_i + \epsilon_{ij}} for \m{i=1,2} and \m{j=1,\dots,12} in the matrix form for the probability model of a linear model, \m{\vec{Y}= \mat{X}\vec{\beta}+ \vec{\epsilon}}. 

\vspace{3mm}

This asks you to write down a choice of \m{\vec{Y}}, \m{\mat{X}}, \m{\vec{\beta}} and \m{\vec{\epsilon}} so that the two equations are equivalent.
 
\answer{\vspace{80mm}}{Concatenating both treatment groups into a single vector, as before, we write
\[
\begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_{12} \\ Y_{13} \\ \vdots \\ Y_{24} \end{bmatrix}
=
\begin{bmatrix} 1 & 0 \\ 1 & 0 \\ \vdots \\ 1 & 0 \\ 0 & 1 \\ \vdots \\ 0 & 1 \end{bmatrix}
\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}
+ 
\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_{12} \\ \epsilon_{13} \\ \vdots \\ \epsilon_{24} \end{bmatrix}
\quad\quad
\mbox{ with $\vec{\epsilon}\sim \MVN(0,\sigma^2\mat{I})$}
\]
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Alternative representations of factors}
\begin{myitemize}
\item 
Consider the following two models in double subscript form, with \m{\epsilon_{ij}\sim\iid\;\normal(0,\sigma)} for \m{i=1,2} and  \m{j=1,\dots,12}.

\vspace{1mm}

\m{\mathrm{(M1).}\quad\quad Y_{ij}= \mu_i + \epsilon_{ij}} 

\vspace{1mm}

\m{\mathrm{(M2).}\quad\quad Y_{ij}= \mu + \alpha_i + \epsilon_{ij} \quad \mbox{with $\alpha_1=0$}}

\vspace{1mm}

\end{myitemize}
\myquestion. Why are (M1) and (M2) equivalent?

\answer{\vspace{25mm}}{Both describe a constant mean for each treatment group with added ``measurement error''. Here, the term $\epsilon_{ij}$ models biological variability not error in the balance used to measure weight.}

\myquestion. What is the difference in the interpretation of the parameters between (M1) and (M2)?

\answer{\vspace{30mm}}{The parameter $\alpha_i$ is called a \myemph{contrast}. It measures the difference between the treament means. We can describe both treatment directly (as in M1) or by specifying the first mean and the contrast (as in M2).}

\end{frame} 
 
\begin{frame}
\frametitle{An over-specified model}
\begin{myitemize}
\item Recall (M2) with \m{\epsilon_{ij}\sim\iid\;\normal(0,\sigma)} for \m{i=1,2} and  \m{j=1,\dots,12}.

\vspace{1mm}

\m{\mathrm{(M2).}\quad\quad Y_{ij}= \mu + \alpha_i + \epsilon_{ij} \quad \mbox{with $\alpha_1=0$}}

\vspace{1mm}

\item Suppose we modify model (M2) to omit the important detail that \m{\alpha_1=0}. This gives

\vspace{1mm}

\m{\mathrm{(M3).}\quad\quad Y_{ij}= \mu + \alpha_i + \epsilon_{ij}}

\vspace{1mm}

\item Many rude words are used to describe the problem with (M3) such as \myemph{over-specified}, \myemph{over-parameterized}, \myemph{unidentifiable}, \myemph{redundant}.

\end{myitemize} 

\vspace{2mm}

\myquestion. Can you see and explain the concern about (M3)?

\answer{\vspace{50mm}}{The parameters are not uniquely specified. For example, we can get a mean of 20 for the first treament group and 25 for the second in 2 ways: ($\mu=10$, $\alpha_1=10$, $\alpha_2=15$) or  ($\mu=15$, $\alpha_1=5$, $\alpha_2=10$). There are infinitely many other equivalent ways.
}

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Using a linear model with factors to test equality of means}

\begin{myitemize}
\item A null hypothesis is that the mice weights for both treatment groups are drawn from the same distribution. Any difference is just chance variation in this particular sample. If the null hypothesis is a plausible description of our data, we don't want to spent too much time interpreting this experimental results.
\item A natural way to write this null hypothesis is \m{H_0:\mu_1=\mu_2} in the model representation (M1)
\item \myemph{A USEFUL TRICK}. Using the equivalent model representation (M2), this becomes \m{H_0: \alpha=0} which is the easiest type of null hypothesis for a linear model.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Factors in \code{lm()}}

\vspace{-1.5mm}

\begin{myitemize}
\item If you give \code{lm()} an explanatory variable of class \code{character} it interprets the variable as levels of a factor.
\end{myitemize}

\vspace{-2mm}

<<>>=
mice <- read.csv(
"https://ionides.github.io/401f18/hw/hw01/femaleMiceWeights.csv"
)
head(mice,3)
lm1 <- lm(Bodyweight~Diet,data=mice)
summary(lm1)$coef
@

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{What model has R actually fitted?}
\begin{myitemize}
\item It can be hard to figure out what R is actually doing when it fits models with a factor.
\item If you can't correctly write the model R is fitting using subscript notation you may well interpret the results wrong.
\item A good check is to look at R's design matrix
\end{myitemize}
<<>>=
model.matrix(lm1)[c(1:2,12:13,23:24),]
@

\end{frame}

%\end{document}

\begin{frame}[fragile]
<<,echo=F>>=
head(model.matrix(lm1),3)
@
\myquestion. Write down the sample model R has fitted, in double subscript form, and interpret the parameters.

\answer{\vspace{60mm}}{$y_{ij}=\mu+\alpha_i+e_{ij}$ with $\alpha_1=0$, for $i=1,2$ and $j=1,\dots,12$. Here, $\mu$ is the mean for treatment group $i=1$ and $\alpha_2$ is the contrast for high fat diet relative to normal diet.
}
\end{frame}

%\end{document}

\begin{frame}[fragile]
<<>>=
summary(lm1)$coef
@
\myquestion. 
Consider the null hypothesis that the two diets are equivalent, so the observed difference in mouse weights is chance variation in the sample. Make both a t-test and a normal approximation test (also known as a z-test) of this hypothesis. Which test do you prefer, and why? A test at the 5\% level is appropriate for this fairly small sample. 

\answer{\vspace{60mm}}{The t-test is made in the above output. We see that a null hypothesis of $H_0: \alpha_2=0$ has a p-value of 0.052 using a 2-sided test comparing the sample t statistics of 2.06 with draws from the t distribution on 22 degrees of freedom. We cannot reject the null hypothesis at the 5\% level. Now we make a z-test using the same statistic: recall that the t-test and z-test use the same statistic, which is the sample estimate divided by its standard error. In this case, since $2.06>1.96$ we reject $H_0$ at the 5\% level. A t-test is more accurate here, since the sample size is small. The evidence is marginal and a hard decision cutoff at 5\% may not make sense.}

\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{A linear model vs a two sample test}
\begin{myitemize}
%\item Testing equality of means is a basic statistical problem. 
\item The linear model test above is equivalent to a \myemph{two sample t-test with pooled variance}. 
<<>>=
t.test(mice$Bodyweight[1:12],mice$Bodyweight[13:24],
  var.equal=TRUE)
@
\item Check that the p-values are the same in both cases.
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Why does the above linear model test match the two sample t-test with pooled variance?}

\begin{myitemize}
\item Tests are the same if they use the same probability model for the null hypothesis and an equivalent test statistic.
\item If you wrote out the probability model justifying the two sample t-test with pooled variance it would be exactly the model (M1) or (M2).
\item  Here we focus on tests via a linear model, but you might like to review two sample tests at
\url{https://open.umich.edu/find/open-educational-resources/statistics/statistics-250-introduction-statistics-data-analysis}
\item Your previous course in statistics likely did not explain the probability model behind the two sample t-test with pooled variance. Viewing this test as a special case of the linear model gives us a way to do that. 
\item The linear model also lets us analyze many more complex models.

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{A factor with many levels: Kicking field goals}
\begin{myitemize}
\item If an athlete has a good season, is the next one likely to be good? Or bad? Or does the previous season have no predictive skill?
\item We consider field goal kicking success for the 19 National Football League (NFL) kickers who played every season during 2002-2006.
\end{myitemize}
<<,eval=F>>=
download.file(destfile="FieldGoals.csv", 
  url="https://ionides.github.io/401f18/07/FieldGoals.csv")
@
<<>>=
goals <- read.table("FieldGoals.csv",header=TRUE,sep=",")
head(goals[,1:8]) 
@
\end{frame}

\begin{frame}[fragile]
\frametitle{Understanding the field goal data}

\vspace{-1.5mm}

<<>>=
goals[1,1:8]
@
\begin{myitemize}
\item Each record has the player \code{Name} and \code{Year} followed by \\
%\begin{myitemize}
%  \item 
\code{Teamt}: team that year. 
\\
\code{FGAt}: number of field goal attempts in that year.
\\
\code{FGt}: percentage of field goal attempts which were successful that year.
\\
\code{Teamt1}: Team the previous year.
\\
\code{FGAt1} and \code{FGt1}: Field goal attempts and percentage the previous year.
\end{myitemize}

\vspace{1.5mm}

\myquestion. Is there additional background on football that we need to understand the data and the question?

\answer{\vspace{40mm}}{Any class suggestions?}

\end{frame}

\begin{frame}
\frametitle{Brainstorming for a model}
\begin{myitemize}
\item If an NFL kicker has a good season, is the next one likely to be good? Or bad? Or does the previous season have no predictive skill?
\end{myitemize}
\myquestion. (1) Set up notation; (2) propose models in the context of our data; (3) write down hypotheses relevant to our question.

\answer{\vspace{60mm}}{Let $y_{ij}$ be the goal kicking percentage for the $i$th kicker ($i=1,\dots,19$) in the $j$th year ($j=1,2,3,4$ corresponding to 2003,2004,2005,2006). Let $p_{ij}$ be the kicking percentage in the previous year, so $p_{ij}=y_{i,j-1}$ with $y_{i0}$ corresponding to 2002. 
Possible models include 
\begin{eqnarray}
y_{ij}&=&m +e_{ij} \\
y_{ij}&=& m + a_i + e_{ij} \\
y_{ij}&=& m + a_i + bp_{ij} + e_{ij}, \quad \mbox{with $a_1=0$}  \label{kicker3} \\
y_{ij}&=& m + a_i + b_i p_{ij} + e_{ij}, \quad \mbox{with $a_1=0$} \label{kicker4} \\
y_{ij}&=& m + a_i + c_j + e_{ij}, \quad \mbox{with $a_1=0$ and $c_1=0$}
\end{eqnarray}
and many more. 
}

\end{frame}

\begin{frame}
\frametitle{Brainstorming continued}

\answer{\vspace{120mm}}{
Data analysis is required to say which is best.
Selecting between candidate models is one of our main tasks for the remainder of this semester. 
Our primary null hypothesis is $\beta=0$ in (\ref{kicker3}) or $\beta_i=0$ for all $i$ in (\ref{kicker4}). 
}

\end{frame}

\begin{frame}[fragile]
\frametitle{A linear model for field goals} 

\vspace{-2mm}

<<>>=
goals.lm <- lm(FGt~FGt1+Name,data=goals)
X <- model.matrix(goals.lm)
@
\begin{myitemize}
\item Here, \code{Name} has R class \code{factor}. The levels are the kicker names.
<<>>=
class(goals$Name)
attributes(goals$Name)$levels[1:6]
@
%\item A factor is a numeric vector with \myemph{levels}. Here, the levels are the kicker names.
\item We want to find out what model we have fitted! Time to study the design matrix, \code{X}.
<<>>=
dim(X)
@
\item Working out the model (in double-subscript form) corresponding to a \m{76\times 20} matrix takes some thought.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
<<>>=
unname(X[1:15,1:10])
@

\end{frame}

\begin{frame}
%\frametitle{

\vspace{5mm}

\myquestion. What is the probability model fitted by \code{lm(FGt~FGt1+Name,data=goals)}? Use double-subscript form.

\answer{\vspace{80mm}}{$Y_{ij}=\mu + \alpha_i + \beta p_{ij} + \epsilon_{ij}$ with $\alpha_1=0$.}

\end{frame}

\begin{frame}[fragile]

\vspace{5mm}

\myquestion. What are the terms in the sample linear model corresponding to the following R output?

<<>>=
coef(goals.lm)[1:6]
@

\answer{\vspace{20mm}}{$m=126.9$, $b=-0.504$, $a_2=-4.64$, $a_3=-3.02$, $\dots$}

\myquestion. Is there anything surprising about these results?

\answer{\vspace{20mm}}{It may be surprisingg that $b$ is negative. It seems that good seasons are usually followed by below-average seasons.}

\end{frame}

\begin{frame}[fragile]
\frametitle{Plotting the fitted model with a line for each factor level}

\vspace{-1.5mm}

<<plot_kickers,echo=T,eval=F,purl=F>>=
plot(FGt~FGt1,data=goals)
intercept <- coef(goals.lm)[1]
slope <- coef(goals.lm)[2]
kicker <- coef(goals.lm)[3:20]
abline(a=intercept,b=slope)
for(i in 1:18) abline(a=intercept+kicker[i],b=slope)
@

\vspace{-10mm}

<<,out.width="55mm",fig.width=3.3,fig.height=4,echo=F>>=
<<plot_kickers>>
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Hypothesis tests for groups of parameters}

\begin{myitemize}
\item We've seen how the least squares coefficient can be used as a test statistic for the null hypothesis that a parameter in a linear model is zero.

\item Sometimes we want to test many parameters at the same time. For example, when analyzing the field goal kicking data, we must decide whether to have a separate intercept for each player. 

\end{myitemize}

\myquestion. There are 19 kickers in the dataset. How many extra parameters are needed if we add an intercept for each player?

\answer{\vspace{15mm}}{18, since we already have an intercept.}

\begin{myitemize}
\item This type of question is called \myemph{model selection}. Our test statistic should compare \myemph{goodness of fit} with and without the additional parameters.

\item We need to know the distribution of the model-generated test statistic under the null hypothesis to find the p-value for the test.

\end{myitemize}

\end{frame}



\begin{frame}[fragile]
\frametitle{Residual sum of squares to quantify goodness of fit}

\vspace{-2mm}

Let \m{\vect{y}} be the data. Let \m{H_0} be a linear model, \m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}}. Let \m{H_a} extend \m{H_0} by adding \m{d} additional explanatory variables. 

\vspace{-1mm}

\begin{myitemize}
\item Let \m{\RSS_0} be the residual sum of squares for \m{H_0}. The residual errors are \m{\vect{e}=\vect{y}-\mat{X}\vect{b}} where \m{\vect{b}=\big(\mat{X}^\transpose\mat{X}\big)^{-1}\mat{X}^\transpose \vect{y}}. So,
\m{
\RSS_0 = \sum_{i=1}^n e_i^2
}.

\vspace{2mm}

\item Let \m{\RSS_a} be the residual sum of squares for \m{H_a}.

\vspace{2mm}

\item Residual sum of squares is a measure of goodness of fit. A small residual sum of squares suggests a model that fits the data well.

\end{myitemize}
\myquestion. It is always true that \m{\RSS_a \le \RSS_0}. Why?

\answer{\vspace{15mm}}{Yes. Adding extra parameters can only reduce the minimized sum of squares. If we set the extra parameters to zero, we get back to the original sum of squares $\RSS_0$.} 

\begin{myitemize}
\item 
We want to know how much smaller \m{\RSS_a} has to be than \m{\RSS_0} to give satisfactory evidence in support of adding the extra explanatory variables into our model. In other words, when should we reject \m{H_0} in favor of \m{H_a}?

\end{myitemize}

\end{frame}



\begin{frame}[fragile]
\frametitle{The f statistic for adding groups of parameters}

\vspace{-1mm}

Formally, we have \m{H_0: \vect{Y}=\mat{X}\vect{\beta}+\epsilon} and \m{H_a: \vect{Y}=\mat{X}_a\vect{\beta}_a+\epsilon}, where \m{\mat{X}} is an \m{n\times p} matrix and \m{\mat{X}_a=[\, \mat{X} \; \mat{Z}\, ]} is an \m{n\times q} matrix with \m{q=p+d}. Here, \m{\mat{Z}} is a \m{n\times d} matrix of additional explanatory variables for \m{H_a}. As usual, we model \m{\epsilon_1,\dots,\epsilon_n} as iid \m{N[0,\sigma]}.

\vspace{-1.5mm}

\begin{myitemize}
\item Consider the following sample test statistic:
\end{myitemize}
\mydisplaymath{
f = \frac{ (\RSS_0-\RSS_a)/d}{\RSS_a/(n-q)}.
}

\begin{myitemize}
\item The denominator is an estimate of \m{\sigma^2} under \m{H_a}. Using this denominator \myemph{standardizes} the test statistic.
\item The numerator \m{ (\RSS_0-\RSS_a)/d} is the \myemph{change in RSS per degree of freedom}. Parameters in linear models are often interpreted as degrees of freedom of the model.
\item Let \m{F} be a model-generated version of \m{f}, with the data \m{\vect{y}} replaced by a random vector \m{\vect{Y}}. If \m{H_0} is true, then the RSS per degree of freedom should be about the same on the numerator and the denominator, so \m{F\approx 1}. Large values, \m{f\gg 1}, are therefore evidence against \m{H_0}. 
\end{myitemize}

\end{frame}



\begin{frame}[fragile]
\frametitle{The F test for model selection}

%\vspace{-2mm}

\begin{myitemize}
\item Under \m{H_0}, the model-generated \m{F} statistic has an F distribution on \m{d} and \m{n-q} degrees of freedom. 
\item Because of the way we constructed the \m{F} statistic, its distribution under \m{H_0} doesn't depend on \m{\sigma}. It only depends on the dimension of \m{\mat{X}} and \m{\mat{X}_a}.

\item We can obtain p-values for the F distribution in R using \code{pf()}. Try \code{?pf}. 

\item Testing \m{H_0} versus \m{H_a} using this p-value is called the F test.

% \item When we add a single parameter, so \m{d=1} and \m{q=p+1}, the F test is equivalent to carrying out Student's t test using the estimated coefficient as the test statistic. 
% As homework, you are asked to check this using \code{pt()} and \code{pf()} in R.

\item Degrees of freedom are mysterious. The mathematics for how they work involves matrix algebra beyond this course. An intuition is that fitting a parameter that is not in the model ``explains'' a share of the residual sum of squares; in an extreme case, fitting \m{n} parameters to \m{n} data points may give a perfect fit (residual sum of squares = zero) even if none of these parameters are in the true model.

\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{The F test is called ``analysis of variance''}

\begin{myitemize}
\item The F test was invented before computers existed.
\item Working out the sums of squares efficiently, by hand, was a big deal!
\item Sums of squares of residuals are relevant for estimating variance. 
\item Building F tests is historically called \myemph{analysis of variance} or abbreviated to \myemph{ANOVA}.
\item The sums of squares and corresponding F tests are presented in an \myemph{ANOVA table}. We will see one in the following data analysis.
\end{myitemize}

\end{frame}



\begin{frame}[fragile]
\frametitle{An F test for kickers: Interpreting the ANOVA table}

\vspace{-2mm}

<<anova>>=
anova(goals.lm)
@

\vspace{-2mm}

\myquestion. Focus on the row labeled \code{Name}. Explain what is being tested, how it is being tested, and what you conclude. In other words, write out the hypothesis test corresponding to this line.

\answer{\vspace{50mm}}{The probability model is as written above. The null hypothesis is $H_0:\alpha_i=0$ for all $i$. The alternative is $H_a:\mbox{$\alpha_i$ are unconstrained}$. 
}
\end{frame}

\begin{frame}[fragile]
\frametitle{ANOVA hypothesis test, continued}

\answer{\vspace{80mm}}{
The sample F statistic is 
\[
f = \frac{(\RSS_0-\RSS_a)/18}{\RSS_a/56} = 3.24. 
\]
Comparing with the F distribution on 18 and 56 degrees of freedom gives a p-value of 0.00039 from the R output.
This is small compared to the 0.001 level. There is compelling evidence that kickers have fundamentally different kicking percentages. To be cautious, we can't necessarily deduce this is a consequence of differing skill. It might involve other aspects such as team strategy.
}

\end{frame}

\end{document}

