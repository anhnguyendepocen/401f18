

\begin{frame}[fragile]
\frametitle{Hypothesis tests for groups of parameters}

\begin{myitemize}
\item We've seen how the least squares coefficient can be used as a test statistic for the null hypothesis that a parameter in a linear model is zero.

\item Sometimes we want to test many parameters at the same time. 

\item A basic hypothesis for this is: Do any of the proposed explanatory variables in a linear model help to explain the response? Or are the data adequately explained by measurement error and a constant expected value?

\end{myitemize}

\vspace{15mm}

\begin{myitemize}
\item This type of question is called \myemph{model selection}. Our test statistic should compare \myemph{goodness of fit} with and without the additional parameters.

\item We need to know the distribution of the model-generated test statistic under the null hypothesis to find the p-value for the test.

\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{Residual sum of squares to quantify goodness of fit}

\vspace{-2mm}

Let \m{\vect{y}} be the data. Let \m{H_0} be a linear model, \m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}}. Let \m{H_a} extend \m{H_0} by adding \m{d} additional explanatory variables. 

\vspace{-1mm}

\begin{myitemize}
\item Let \m{\RSS_0} be the residual sum of squares for \m{H_0}. The residual errors are \m{\vect{e}=\vect{y}-\mat{X}\vect{b}} where \m{\vect{b}=\big(\mat{X}^\transpose\mat{X}\big)^{-1}\mat{X}^\transpose \vect{y}}. So,
\m{
\RSS_0 = \sum_{i=1}^n e_i^2
}.

\vspace{2mm}

\item Let \m{\RSS_a} be the residual sum of squares for \m{H_a}.

\vspace{2mm}

\item Residual sum of squares is a measure of goodness of fit. A small residual sum of squares suggests a model that fits the data well.

\end{myitemize}
\myquestion. It is always true that \m{\RSS_a \le \RSS_0}. Why?

\vspace{15mm}


\begin{myitemize}
\item 
We want to know how much smaller \m{\RSS_a} has to be than \m{\RSS_0} to give satisfactory evidence in support of adding the extra explanatory variables into our model. In other words, when should we reject \m{H_0} in favor of \m{H_a}?

\end{myitemize}

\end{frame}



\begin{frame}[fragile]
\frametitle{The f statistic for adding groups of parameters}

\vspace{-1mm}

Formally, we have \m{H_0: \vect{Y}=\mat{X}\vect{\beta}+\epsilon} and \m{H_a: \vect{Y}=\mat{X}_a\vect{\beta}_a+\epsilon}, where \m{\mat{X}} is an \m{n\times p} matrix and \m{\mat{X}_a=[\, \mat{X} \; \mat{Z}\, ]} is an \m{n\times q} matrix with \m{q=p+d}. Here, \m{\mat{Z}} is a \m{n\times d} matrix of additional explanatory variables for \m{H_a}. As usual, we model \m{\epsilon_1,\dots,\epsilon_n} as iid \m{N[0,\sigma]}.

\vspace{-1.5mm}

\begin{myitemize}
\item Consider the following sample test statistic:
\end{myitemize}
\mydisplaymath{
f = \frac{ (\RSS_0-\RSS_a)/d}{\RSS_a/(n-q)}.
}

\begin{myitemize}
\item The denominator is an estimate of \m{\sigma^2} under \m{H_a}. Using this denominator \myemph{standardizes} the test statistic.
\item The numerator \m{ (\RSS_0-\RSS_a)/d} is the \myemph{change in RSS per degree of freedom}. Parameters in linear models are often interpreted as degrees of freedom of the model.
\item Let \m{F} be a model-generated version of \m{f}, with the data \m{\vect{y}} replaced by a random vector \m{\vect{Y}}. If \m{H_0} is true, then the RSS per degree of freedom should be about the same on the numerator and the denominator, so \m{F\approx 1}. Large values, \m{f\gg 1}, are therefore evidence against \m{H_0}. 
\end{myitemize}

\end{frame}



\begin{frame}[fragile]
\frametitle{The F test for model selection}

\vspace{-2mm}

\begin{myitemize}
\item Under \m{H_0}, the model-generated \m{F} statistic has an F distribution on \m{d} and \m{n-q} degrees of freedom. 
\item Because of the way we constructed the \m{F} statistic, its distribution under \m{H_0} doesn't depend on \m{\sigma}. It only depends on the dimension of \m{\mat{X}} and \m{\mat{X}_a}.

\item We can obtain p-values for the F distribution in R using \code{pf()}. Try \code{?pf}. 

\item Testing \m{H_0} verus \m{H_a} using this p-value is called the F test.

\item When we add a single parameter, so \m{d=1} and \m{q=p+1}, the F test is equivalent to carrying out Student's t test using the estimated coefficient as the test statistic. 
As homework, you are asked to check this using \code{pt()} and \code{pf()} in R.

\item Degrees of freedom are mysterious. The mathematics for how they work involves matrix algebra beyond this course. An intuition is that fitting a parameter that is not in the model ``explains'' a share of the residual sum of squares; in an extreme case, fitting \m{n} parameters to \m{n} data points may give a perfect fit (residual sum of squares = zero) even if none of these parameters are in the true model.

\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{The F test is called ``analysis of variance''}

\begin{myitemize}
\item The F test was invented before computers existed.
\item Working out the sums of squares efficiently, by hand, was a big deal!
\item Sums of squares of residuals are relevant for estimating variance. 
\item Building F tests is historically called \myemph{analysis of variance} or abbreviated to \myemph{ANOVA}.
\item The sums of squares and corresponding F tests are presented in an \myemph{ANOVA table}. We will see one in the following data analysis.
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{An F test for kickers. (i) Reviewing the data}
<<data>>=
goals <- read.table("FieldGoals2003to2006.csv",header=T,sep=",")
goals[1:5,c("Name","Teamt","FGt","FGtM1")]
lm0 <- lm(FGt~FGtM1+Name,data=goals)
@
\begin{myitemize}
\item This is model syntax we have not seen before. 
\item \code{Name} is a \myemph{factor}
<<factor_class>>=
class(goals$Name)
@
\item A factor is a vector with \myemph{levels}. Here, the levels are the kicker names.
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{An F test for kickers. (ii) Checking the design matrix}

\vspace{-2mm}

<<design>>=
X <- model.matrix(lm0)
dim(X)
unname(X[c(1,5,9,13,17),1:8])
@

\vspace{-3mm}

\myquestion. Is this the design matrix that you want? Can we use our experience working with design matrices to understand what R is doing?

\vspace{15mm}

\end{frame}



\begin{frame}[fragile]
\frametitle{An F test for kickers. (ii) Interpreting the ANOVA table}

\vspace{-2mm}

<<anova>>=
anova(lm0)
@

\vspace{-3mm}

\myquestion. Focus on the row labeled \code{Name}. Explain what is being tested, how it is being tested, and what you conclude.

\vspace{30mm}

\end{frame}

