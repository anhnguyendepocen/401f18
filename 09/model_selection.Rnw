
\begin{frame}[fragile]
\frametitle{Selecting from many possible models}

\vspace{-1mm}

\begin{myitemize}
\item Suppose we have a large number \m{\ell} of potential explanatory variables in our dataset. 
\item The total number of possible linear models is \m{2^\ell} since each of the \m{\ell} variables can be either in or out of the model.
\item If we allow for the possiblity of interactions, things are even worse. 
\item For two variables \m{x_{i1}} and \m{x_{i2}} on each individual \m{i=1,2,\dots,n}, modeling an \myemph{interaction} can be viewed as including a new variable \m{x_{i3}=x_{i1}x_{i2}}.
\end{myitemize}
\myquestion. If there are \m{\ell} explanatory variables, considered as \myemph{main effects}, and any pair of them could give rise to an \myemph{interaction effect}, how many possible models are there? For simplicity, allow for the possibility of including interactions without the main effects.

\answer{\vspace{30mm}}{There are $\ell(\ell-1)/2$ possible interactions and $\ell$ main effects, so $\ell(\ell+1)/2$ possible explanatory variables. Each can be in or out of the model, giving a total of
\[
2^{\ell(\ell+1)/1}
\]
possible models. This get big quickly as $\ell$ increases.
}

\end{frame}


\begin{frame}[fragile]
\frametitle{Practical considerations for model selection}

%\vspace{-2mm}

\begin{myitemize}
\item Sometimes, you build models based on specific hypotheses about the system you are investigating. 
\item In this case, our tools for hypothesis testing work well. You work through a process of starting with a basic model and considering a relatively small sequence of alternative hypotheses to build up an understanding of the data.
\item A different scenario occurs when you explore a very large number of different models. 
\item If you consider \m{1000} alternative models and each one is tested at significance level \m{0.01} then you expect to find \m{10} models that would formally let you reject the null hypothesis at a ``high'' level of significance for random variables generated under the null model.

\item Similar issues arise if you consider many variables in a single linear model and look to identify significant ones.
\end{myitemize}


\end{frame}

\begin{frame}
\frametitle{The expected number of false discoveries}

\myquestion.  Suppose that you consider \m{\ell=100} variables by placing them all in a linear model and reporting the variables whose t statistic is significant at the \m{0.05} level. How many ``significant'' variables would you expect to report under a null probability model where all the coefficients are zero?

\answer{\vspace{50mm}}{If all coefficients are zero, then the chance of a p-value for each variable being significant at the 5\% level (if the null hypothesis is correct) is 5\%. So, the expected number of significant variables is $0.05\times 100=5$.}

\end{frame}


\begin{frame}[fragile]
\frametitle{Confidence intervals after model selection}

\myquestion. Suppose you have \m{\ell=100} explanatory variables and you consider \m{\ell=100} different models, each with only one of the explanatory variables in the model. You pick as your favorite model the one with the highest \m{R^2} statistic, which is equivalent to picking the one with the smalles p-value for its \m{t} statistic. You report a 95\% confidence interval for the coefficient in this linear model. What is the chance that a corresponding model-generated confidence interval will cover the true parameter value, for the null probability model where all the coefficients for all the explanatory variables are zero? You can suppose that, under this null probability model, the model-generated p-values for each explanatory variable are independent.

\answer{\vspace{50mm}}{The 95\% CI covers zero exactly when we fail to reject the null hypothesis that zero is the true value at the 5\% level. Thus, the CI for the parameter with the smallest p-value covers the true value of zero only when all p-values are greater than 0.05. The chance that all model-generated p-values are greater than 0.05 under the null hypothesis is $0.95^{100}=0.0059$.}

\end{frame}

\begin{frame}[fragile]
\frametitle{Dealing with multiple testing}

\begin{myitemize}
\item The difficulty of properly evaluating statistical significance when investigating very many hypotheses is called the \myemph{multiple testing} situation.
\item Dealing with multiple testing is a current scientific concern. It is related to the so-called crisis in scientific reproducibility.
\item Advances in data acquisition and computation increasingly lead to large datasets to be investigated.
\item One principle: report all the tests you make, not just the nominally significant ones. This lets the reader assess the hazard of multiple testing bias.
\item Another principle: any result not yet confirmed by an independent experiment is suspicious.
\end{myitemize}

\end{frame}

