%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{5}
%\newcommand\LSi{\mathrm{(LS1)}}
%\newcommand\LSii{\mathrm{(LS2)}}
%\newcounter{tXX}
%\newcounter{tXy}
%\newcounter{matrixLSi}
\newcounter{CovSum}
\newcounter{CovSumII}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space
<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=T,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
# library(broman) # used for myround 
@


\begin{frame}
\frametitle{Chapter \CHAPTER. Vector random variables}

\vspace{-2mm}

\begin{myitemize}
\item
A \myemph{vector random variable} \m{\vec{X}=(X_1,X_2,\dots,X_n)} is a collection of random numbers with probabilities assigned to outcomes.
\item
\m{\vec{X}} can also be called a \myemph{multivariate random variable}.
\item
The case with \m{n=2} we call a \myemph{bivariate random variable}.
\item Saying \m{X} and \m{Y} are \myemph{jointly distributed random variables} is equivalent to saying \m{(X,Y)} is a bivariate random variable.
\item Vector random variables let us model relationships between quantities.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: midterm and final scores}

\begin{myitemize}
\item 
We will look at the anonymized test scores for a previous course. 
<<download life expectancy data file,eval=F>>=
download.file(destfile="course_progress.txt",
 url="https://ionides.github.io/401f18/01/course_progress.txt")
@
\begin{verbatim}
# Anonymized scores for a random subset of 50 students
"final" "quiz" "hw" "midterm"
"1" 52.3 76.7 91 61.7
"2" 68.2 65.4 94.5 48.3
"3" 78.4 91.2 95.5 80
\end{verbatim}
\end{myitemize}
\begin{columns}[T]
\begin{column}{0.45\textwidth}

\vspace{-1.5mm}

\begin{myitemize}
\item
A probability model lets us answer a question like, ``What is the probability that someone gets at least 70\% in both the midterm and the final''
\end{myitemize}

\vspace{-3mm}

<<scores_plot,eval=F,purl=T>>=
x <- read.table("course_progress.txt")
plot(final~midterm,data=x)
@
\end{column}
\begin{column}{0.5\textwidth}
\vspace{-30mm}
<<,out.width="55mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F>>=
<<scores_plot>>
@
\end{column}
\end{columns}


\end{frame}

\begin{frame}[fragile]
\frametitle{The bivariate normal distribution and covariance}

\vspace{-1.5mm}

\begin{myitemize}
\item
Let \m{X\sim \normal(\mu_X,\sigma_X)} and \m{Y\sim\normal(\mu_Y,\sigma_Y)}.
\item 
If \m{X} and \m{Y} are bivariate random variables we need another parameter to describe their dependence. If \m{X} is big, does \m{Y} tend to be big, or small, or does the value of \m{X} make no difference to the outcome of \m{Y}?
\item This parameter is the \myemph{covariance}, defined to be
\mydisplaymath{
\cov(X,Y)=\E \Big[ \big( X-\E[X]\, \big) \, \big( Y-\E[Y] \, \big) \Big]
}

\item The parameters of the bivariate normal distribution in matrix form are
the \myemph{mean vector} \m{\vec{\mu}=(\mu_X,\mu_Y)} and the \myemph{variance/covariance matrix},
\mydisplaymath{
\mat{V}=\left[
\begin{array}{cc}
\var(X) & \cov(X,Y) \\
\cov(Y,X) & \var(Y)
\end{array} \right]
}
\item In R, the \code{mvtnorm} package lets us simulate the bivariate and multivariate normal distribution via the \code{rmvnorm()} function. It has the mean vector and variance/covariance matrix as arguments.

\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Experimenting with the bivariate normal distribution}

<<mvn_plot,eval=F,echo=F,purl=T>>=
library(mvtnorm)
mvn <- rmvnorm(n=50,
  mean=c(X=65,Y=65),
  sigma=matrix(
    c(200,100,100,150),
    2,2)
)
plot(Y~X,data=mvn)
@

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\vspace{-15mm}
<<,out.width="50mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F,cache=F>>=
set.seed(21)
<<mvn_plot>>
@
\end{column}
\begin{column}{0.5\textwidth}

\vspace{-3mm}

<<,eval=F,echo=T,purl=F>>=
<<mvn_plot>>
@
\end{column}
\end{columns}

\vspace{-3mm}

\begin{myitemize}
\item We write \m{(X,Y)\sim \MVN(\vec{\mu},\mat{V})}, where MVN is read "multivariate normal".
\end{myitemize}
\myquestion. What are \m{\mu_X}, \m{\mu_Y}, \m{\var(X)}, \m{\var(Y)}, and \m{\cov(X,Y)} for this simulation?

\answer{\vspace{20mm}}{TODO}

\end{frame}

\begin{frame}[fragile]
\frametitle{The bivariate normal as a model for exam scores}
%\begin{myitemize}
%\item 

\myquestion. Compare the data on midterm and final scores with the simulation. Does a normal model seem to fit? Would you expect it to? Why, and why not?





\begin{columns}[T]
\begin{column}{0.55\textwidth}
\end{column}
\begin{column}{0.4\textwidth}

\vspace{-15mm}
<<,out.width="38mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F>>=
<<scores_plot>>
@

\vspace{-10mm}

<<,out.width="38mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F,cache=F>>=
set.seed(21)
<<mvn_plot>>
@
\end{column}
\end{columns}


\answer{\vspace{40mm}}{TODO}

\end{frame}


\begin{frame}[fragile]
\frametitle{More on covariance}
\begin{myitemize}
\item Covariance is \myemph{symmetric}: we see from the definition
\mydisplaymath{
%\hspace{-2mm}
\begin{array}{rcccl}
\vspace{4mm}
\cov(X,Y)&=&\E \Big[ \big( X-\E[X]\, \big) \, \big( Y-\E[Y] \, \big) \Big] &&
\\
&=& \E \Big[ \big( Y-\E[Y]\, \big) \, \big( X-\E[X] \, \big) \Big] &=& \cov(Y,X)
\end{array}
}
\item Also, we see from the definition that \m{\cov(X,X)=\var(X)}.
\item The \myemph{sample covariance} of \m{n} pairs of measurements \m{(x_1,y_1),\dots,(x_n,y_n)} is
\mydisplaymath{
\covSample(\vec{x},\vec{y}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)(y_i -\bar y)
}
where \m{\bar x} and \m{\bar y} are the sample means of \m{\vec{x}=(x_1,\dots,x_n)} and \m{\vec{y}=(y_1,\dots,y_n)}.

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Scaling covariance to give correlation}

\vspace{-1mm}

\begin{myitemize}

\item The standard deviation of a random variable is interpretable as its scale.
\item Variance is interpretable as the square of standard deviation
<<,eval=T>>=
var(x$midterm)
var(x$final)
cov(x$midterm,x$final)
@
\item Covariance is interpretable when scaled to give the \myemph{correlation}
\mydisplaymath{
\hspace{-2mm}
\cor(X,Y)=\frac{\cov(X,Y)}{\sqrt{\var(X)\var(Y)}} 
\hspace{20mm}
\corSample(\vec{x},\vec{y})=\frac{\covSample(\vec{x},\vec{y})}{\sqrt{\varSample(\vec{x})\varSample(\vec{y})}}
} 
<<,eval=T>>=
cor(x$midterm,x$final)
@
\end{myitemize}

\end{frame}


\begin{frame}[fragile]

<<mvn_cor_plot,eval=F,echo=F,purl=T,cache=F>>=
library(mvtnorm)
mvn <- rmvnorm(n=100,
  mean=c(X=0,Y=0),
  sigma=matrix(
    c(1,rho,rho,1),
    2,2)
)
@


\begin{columns}[T]
\begin{column}{0.45\textwidth}
<<,eval=T,cache=F>>=
rho <- 0
@
\vspace{-12mm}
<<,out.width="45mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F,cache=F>>=
set.seed(79)
<<mvn_cor_plot>>
plot(X~Y,data=mvn,xlab="",ylab="")
@

\vspace{-10mm}

<<,echo=T>>=
<<mvn_cor_plot>>
@
\end{column}

\begin{column}{0.45\textwidth}
<<,eval=T,cache=F>>=
rho <- -0.8
@
\vspace{-12mm}
<<,eval=T,out.width="45mm",fig.width=3.3,fig.height=4,echo=F,purl=F,cache=F>>=
set.seed(81)
<<mvn_cor_plot>>
plot(X~Y,data=mvn,xlab="",ylab="")
@

\vspace{-10mm}

<<,eval=T>>=
rho <- 0.95
@
\vspace{-12mm}
<<,eval=T,out.width="45mm",fig.width=3.3,fig.height=4,echo=F,purl=F>>=
set.seed(82)
<<mvn_cor_plot>>
plot(X~Y,data=mvn,xlab="",ylab="")
@

\end{column}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{More on interpreting correlation}

\begin{myitemize}
\item Random variables with a correlation of $\pm 1$ (or data with a sample correlation of $\pm 1$) are \myemph{linearly dependent}.
\item Random variables with a correlation of 0 (or data with a sample correlation of 0) are \myemph{uncorrelated}.
\item Random variables with a covariance of 0 are also uncorrelated!

\end{myitemize}

\myquestion. Suppose two data vectors \m{\vec{x}=(x_1,\dots,x_n)} and \m{\vec{y}=(y_1,\dots,y_n)} have been \myemph{standardized}. That is, each data point has had the sample mean substracted and then been divided by the sample standard deviation. You calculate \m{\covSample(\vec{x},\vec{y})=0.8}. What is the sample correlation, \m{\corSample(\vec{x},\vec{y})}?

\answer{\vspace{25mm}}

\end{frame}

\begin{frame}[fragile]
\frametitle{The variance of a sum}
\begin{myitemize}
\item A basic property of covariance is

\vspace{1.5mm}

\m{\mbox{(Eq. C1)}\hspace{20mm}
\var(X+Y)=\var(X)+\var(Y) + 2\, \cov(X,Y)
}

\vspace{1mm}

\item Sample covariance has the same formula,

\vspace{1.5mm}

\m{\mbox{(Eq. C2)}\hspace{20mm}
\varSample(\vec{x}+\vec{y})=\varSample(\vec{x})+\varSample(\vec{y}) + 2\, \covSample(\vec{x},\vec{y})
}

\vspace{1mm}


\item These nice formulas mean it can be easier to calculate using variances and covariances rather than standard deviations and correlations. 

\end{myitemize}

\myquestion. Rewrite (Eq. C1) to give a formula for \m{\sd(X+Y)} in terms of \m{\sd(X)}, \m{\sd(Y)} and \m{\cor(X,Y)}.

\answer{\vspace{30mm}}{TODO}

\end{frame}

\begin{frame}[fragile]
\frametitle{More properties of covariance}
\begin{myitemize}
\item Covariance is not affected by adding constants to either variable

\vspace{2mm}

\m{\mbox{(Eq. C3)}\hspace{20mm}
\cov(X+a,Y+b)=\cov(X,Y)
}

\vspace{1mm}

\item Recall the definition \m{\cov(X,Y)=\E \big[ \big( X-\E[X]\, \big) \, \big( Y-\E[Y] \, \big) \big]}.
In words, covariance is the mean product of deviations from average. These deviations are unchanged when we add a constant to the variable.

\item Covariance scales \myemph{bilinearly} with each variable

\vspace{2mm}

\m{\mbox{(Eq. C3)}\hspace{20mm}
\cov(aX,bY)=ab \, \cov(X,Y)
}

\vspace{1mm}

\item Covariance distributes across sums

\vspace{2mm}

\m{\mbox{(Eq. C4)}\hspace{20mm}
\cov(X,Y+Z)= \cov(X,Y) + \cov(X,Z)
}

\vspace{1mm}

\item Sample covariances also have these properties. You can test them in R using bivariate normal random variables, constructed as previously using `rmvnorm()`.

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{The variance/covariance matrix of vector random variables}

\vspace{-1.5mm}

\begin{myitemize}
\item Let \m{\vec{X}=(X_1,\dots,X_p)} be a vector random variable. For any pair of elements, say \m{X_i} and \m{X_j}, we can compute the usual scalar covariance, \m{v_{ij}=\cov(X_i,X_j)}.
\item The variance/covariance matrix \m{\mat{V}=[v_{ij}]_{p\times p}} collects together all these covariances.
\mydisplaymath{
\mat{V}
=
\var(\vec{X})
=
\left[ 
\begin{array}{cccc}
\cov(X_1,X_1) & \cov(X_1,X_2) & \dots & \cov(X_1,X_p) 
\\
\cov(X_2,X_1) & \cov(X_2,X_2) &  & \cov(X_2,X_p) 
\\
\vdots &  & \ddots & \vdots
\\
\cov(X_p,X_1) & \cov(X_p,X_2) & \dots & \cov(X_p,X_p) 
\end{array}
\right]
}
\item The diagonal entries of \m{\mat{V}} are \m{v_{ii}=\cov(X_i,X_i) = \var(X_i)} for \m{i=1,\dots,p} so the variance/covariance matrix can be written as
\mydisplaymath{
\mat{V}
=
\var(\vec{X})
=
\left[ 
\begin{array}{cccc}
\var(X_1) & \cov(X_1,X_2) & \dots & \cov(X_1,X_p) 
\\
\cov(X_2,X_1) & \var(X_2) &  & \cov(X_2,X_p) 
\\
\vdots &  & \ddots & \vdots
\\
\cov(X_p,X_1) & \cov(X_p,X_2) & \dots & \var(X_p) 
\end{array}
\right]
}

\end{myitemize}

\end{frame}


\begin{frame}
\frametitle{The correlation matrix}
\begin{myitemize}
\item Covariance is harder to interpret than correlation, but easier for calculations.
\item We can put together all the correlations into a correlation matrix, using the fact that \m{\cor(X_i,X_i)=1}.
\mydisplaymath{
\cor(\vec{X})=\left[ 
\begin{array}{cccc}
1 & \cor(X_1,X_2) & \dots & \cor(X_1,X_p) 
\\
\cor(X_2,X_1) & 1 &  & \cor(X_2,X_p) 
\\
\vdots &  & \ddots & \vdots
\\
\cor(X_p,X_1) & \cor(X_p,X_2) & \dots & 1 
\end{array}
\right]
}
\item Multivariate distributions can be very complicated.
\item The variance/covariance and correlation matrices deal only with \myemph{pairwise} relationships between variables.
\item Pairwise relationships can be graphed.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The sample variance/covariance matrix}

\vspace{-1mm}

\begin{myitemize} 
\item The \myemph{sample variance/covariance matrix} places all the sample variances and covariances in a matrix.
\item Let \m{\mat{X}=[x_{ij}]_{n\times p}} be a data matrix made up of \m{p} data vectors \m{\vec{x}_1, \vec{x}_2, \dots, \vec{x_p}} each of length \m{n}.
\mydisplaymath{
\varSample(\mat{X})
=
\left[ 
\begin{array}{cccc}
\varSample(\vec{x}_1) & \covSample(\vec{x}_1,\vec{x}_2) & \dots & \covSample(\vec{x}_1,\vec{x}_p) 
\\
\covSample(\vec{x}_2,\vec{x}_1) & \varSample(\vec{x}_2) &  & \covSample(\vec{x}_2,\vec{x}_p) 
\\
\vdots &  & \ddots & \vdots
\\
\covSample(\vec{x}_p,\vec{x}_1) & \covSample(\vec{x}_p,\vec{x}_2) & \dots & \varSample(\vec{x}_p) 
\end{array}
\right]
}
\item R uses the same notation. If \code{x} is a matrix or dataframe, \code{var(x)} returns the sample variance/covariance matrix.
<<,echo=T,eval=T>>=
var(x)
@
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The sample correlation matrix}

\vspace{-1mm}

\begin{myitemize} 
\item The \myemph{sample correlation matrix} places all the sample correlations in a matrix.
\item Let \m{\mat{X}=[x_{ij}]_{n\times p}} be a data matrix made up of \m{p} data vectors \m{\vec{x}_1, \vec{x}_2, \dots, \vec{x_p}} each of length \m{n}.
\mydisplaymath{
\corSample(\mat{X})
=
\left[ 
\begin{array}{cccc}
1 & \corSample(\vec{x}_1,\vec{x}_2) & \dots & \corSample(\vec{x}_1,\vec{x}_p) 
\\
\corSample(\vec{x}_2,\vec{x}_1) & 1 &  & \corSample(\vec{x}_2,\vec{x}_p) 
\\
\vdots &  & \ddots & \vdots
\\
\corSample(\vec{x}_p,\vec{x}_1) & \corSample(\vec{x}_p,\vec{x}_2) & \dots & 1 
\end{array}
\right]
}
\item R uses the same notation. If \code{x} is a matrix or dataframe, \code{cor(x)} returns the sample correlation matrix.
<<,echo=T,eval=T>>=
cor(x)
@
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
<<scores_pairs,eval=F,echo=T>>=
pairs(x)
@

\vspace{-5mm}

<<,out.width="95mm",fig.width=5.5,fig.height=5.5,eval=T,echo=F>>=
<<scores_pairs>>
@

\end{frame}

\begin{frame}[fragile]
<<mvn_pairs,eval=F,echo=T>>=
mvn <- rmvnorm(50,mean=apply(x,2,mean),sigma=var(x))
pairs(mvn)
@

\vspace{-5mm}

<<,out.width="90mm",fig.width=5.5,fig.height=5.5,eval=T,echo=F>>=
set.seed(70)
<<mvn_pairs>>
@

\end{frame}

\begin{frame}[fragile]
\myquestion. From looking at the scatterplots, what are the strengths and weaknesses of a multivariate normal model for test scores in this course?

\answer{\vspace{30mm}}{TODO}

\myquestion. To what extent is it appropriate to summarize the data by the mean and variance/covariance matrix (or correlation matrix) when the normal approximation is dubious?

\answer{\vspace{30mm}}{TODO}

\end{frame}

\begin{frame}[fragile]
%\frametitle{Variance/covariance of a linear combination}
\frametitle{Linear combinations}

\vspace{-1.5mm}

\begin{myitemize}
\item Let \m{\vec{X}=(X_1,\dots,X_p)} be a vector random variable with mean vector \m{\vec{\mu}=(\mu_1,\dots,\mu_p)} and \m{p\times p} variance/covariance matrix \m{\mat{V}}.
\item Let \m{\mat{X}} be a \m{n\times p} data matrix.
\item Let \m{\mat{A}} be a \m{q\times p} matrix.
\item \m{\vec{Z}=\mat{A}\vec{X}} is a collection of \m{q} linear combinations of the \m{p} random variables in the vector \m{\vec{X}}, viewed as a \myemph{column} vector.
\item \m{\mat{Z}=\mat{X}\mat{A}^\transpose} is an \m{n\times q} collection of linear combinations of the \m{p} data points in each \myemph{row} of \m{\mat{X}}.
\item Mental gymnastics are required: vectors are often interpreted as \myemph{column vectors} (e.g., \m{p\times 1} matrices) but the vector of measurements for each unit is a \myemph{row vector} when considered as a row of an \m{n\times p} data matrix. 

\end{myitemize}

\vspace{2.5mm}

\myquestion. How would you construct a simulated data matrix \m{\mat{Z}_\mathrm{sim}} from \m{n} realizations \m{\vec{Z}_1,\dots,\vec{Z}_n} of the random column vector \m{\vec{Z}=\mat{A}\vec{X}}? Be careful with transposes and keep track of dimensions.

\end{frame}

\begin{frame}
\answer{{\bf Solution}:

\vspace{35mm}}{TODO}

\begin{myitemize}
\item There is a useful matrix variance/covariance formula for a linear combination.

\vspace{-1mm}

\mydisplaymath{
\var(\mat{A}\, \vec{X}) = \mat{A}\, \var(\vec{X})\, \mat{A}^\transpose 
\hspace{20mm}
\varSample(\mat{X}\, \mat{A}^\transpose) = \mat{A}\, \varSample(\mat{X})\, \mat{A}^\transpose
}
\end{myitemize}

\vspace{5mm}

\myquestion. Add dimensions to each term in these equations to check they make sense.

\answer{\vspace{15mm}}{TODO}
\end{frame}

\begin{frame}[fragile]
\frametitle{Testing the variance/covariance formula}

\vspace{-1mm}

\begin{myitemize}
\item Suppose that the overall course score is weighted 40\% on the final and 20\% on each of the miterm, homework and quiz.
\item We can find the sample variance of the overall score two different ways. 
\end{myitemize}

\vspace{1mm}

(i) Directly computing the overall score for each student.
<<>>= 
weights <- c(final=0.4,quiz=0.2,hw=0.2,midterm=0.2)
overall <- as.matrix(x) %*% weights
var(overall)
@
(ii) Using \m{\varSample(\mat{X}\, \mat{A}^\transpose) = \mat{A}\, \varSample(\mat{X})\, \mat{A}^\transpose}.
<<>>=
weights %*% var(x) %*% weights
@

\vspace{-1mm}

\begin{myitemize}
\item R interprets the vector `weights` as a row or column vector as necessary.
\end{myitemize}
\end{frame}

\end{document}


