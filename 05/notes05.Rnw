%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{5}
\newcounter{CovSum}
\newcounter{CovSumII}

% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
% \newcommand\answer[2]{\textcolor{red}{#2}} % to show answers
 \newcommand\answer[2]{#1} % to show blank space
<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=T,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
# library(broman) # used for myround 
@


\begin{frame}
\frametitle{Chapter \CHAPTER. Vector random variables}

\vspace{-2mm}

\begin{myitemize}
\item
A \myemph{vector random variable} \m{\vec{X}=(X_1,X_2,\dots,X_n)} is a collection of random numbers with probabilities assigned to outcomes.
\item
\m{\vec{X}} can also be called a \myemph{multivariate random variable}.
\item
The case with \m{n=2} we call a \myemph{bivariate random variable}.
\item Saying \m{X} and \m{Y} are \myemph{jointly distributed random variables} is equivalent to saying \m{(X,Y)} is a bivariate random variable.
\item Vector random variables let us model relationships between quantities.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: midterm and final scores}

\begin{myitemize}
\item 
We will look at the anonymized test scores for a previous course. 
<<download life expectancy data file,eval=F>>=
download.file(destfile="course_progress.txt",
 url="https://ionides.github.io/401f18/05/course_progress.txt")
@
\begin{verbatim}
# Anonymized scores for a random subset of 50 students
"final" "quiz" "hw" "midterm"
"1" 52.3 76.7 91 61.7
"2" 68.2 65.4 94.5 48.3
"3" 78.4 91.2 95.5 80
\end{verbatim}
\end{myitemize}
\begin{columns}[T]
\begin{column}{0.45\textwidth}

\vspace{-1.5mm}

\begin{myitemize}
\item
A probability model lets us answer a question like, ``What is the probability that someone gets at least 70\% in both the midterm and the final''
\end{myitemize}

\vspace{-3mm}

<<scores_plot,eval=F,purl=T>>=
x <- read.table("course_progress.txt")
plot(final~midterm,data=x)
@
\end{column}
\begin{column}{0.5\textwidth}
\vspace{-30mm}
<<,out.width="55mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F>>=
<<scores_plot>>
@
\end{column}
\end{columns}


\end{frame}

\begin{frame}[fragile]
\frametitle{The bivariate normal distribution and covariance}

\vspace{-1.5mm}

\begin{myitemize}
\item
Let \m{X\sim \normal(\mu_X,\sigma_X)} and \m{Y\sim\normal(\mu_Y,\sigma_Y)}.
\item 
If \m{X} and \m{Y} are bivariate random variables we need another parameter to describe their dependence. If \m{X} is big, does \m{Y} tend to be big, or small, or does the value of \m{X} make no difference to the outcome of \m{Y}?
\item This parameter is the \myemph{covariance}, defined to be
\mydisplaymath{
\cov(X,Y)=\E \Big[ \big( X-\E[X]\, \big) \, \big( Y-\E[Y] \, \big) \Big]
}

\item The parameters of the bivariate normal distribution in matrix form are
the \myemph{mean vector} \m{\vec{\mu}=(\mu_X,\mu_Y)} and the \myemph{variance/covariance matrix},
\mydisplaymath{
\mat{V}=\left[
\begin{array}{cc}
\var(X) & \cov(X,Y) \\
\cov(Y,X) & \var(Y)
\end{array} \right]
}
\item In R, the \code{mvtnorm} package lets us simulate the bivariate and multivariate normal distribution via the \code{rmvnorm()} function. It has the mean vector and variance/covariance matrix as arguments.

\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Experimenting with the bivariate normal distribution}

<<mvn_plot,eval=F,echo=F,purl=T>>=
library(mvtnorm)
mvn <- rmvnorm(n=50,
  mean=c(X=65,Y=65),
  sigma=matrix(
    c(200,100,100,150),
    2,2)
)
plot(Y~X,data=mvn)
@

\begin{columns}[T]
\begin{column}{0.45\textwidth}
\vspace{-15mm}
<<,out.width="50mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F,cache=F>>=
set.seed(21)
<<mvn_plot>>
@
\end{column}
\begin{column}{0.5\textwidth}

\vspace{-3mm}

<<,eval=F,echo=T,purl=F>>=
<<mvn_plot>>
@
\end{column}
\end{columns}

\vspace{-3mm}

\begin{myitemize}
\item We write \m{(X,Y)\sim \MVN(\vec{\mu},\mat{V})}, where MVN is read "multivariate normal".
\end{myitemize}
\myquestion. What are \m{\mu_X}, \m{\mu_Y}, \m{\var(X)}, \m{\var(Y)}, and \m{\cov(X,Y)} for this simulation?

\answer{\vspace{20mm}}{
The mean vector is $\vec{mu}=(\mu_X,\mu_Y)=(65,65)$.
The variance/covariance matrix is 
$\left[
\begin{array}{cc}
\var(X) & \cov(X,Y) \\
\cov(Y,X) & \var(Y)
\end{array} 
\right]
=
\left[
\begin{array}{cc}
200 & 100 \\
100 & 150
\end{array} 
\right]$
}
\end{frame}

\begin{frame}[fragile]
\frametitle{The bivariate normal as a model for exam scores}

\myquestion. Compare the data on midterm and final scores with the simulation. Does a normal model seem to fit? Would you expect it to? Why, and why not?



\begin{columns}[T]
\begin{column}{0.55\textwidth}

\answer{\vspace{40mm}}{
An exam is made up of many questions and parts of questions, so chance variability can follow a normal curve. Maybe there is some inherent skill that could follow a normal curve for similar reasons, but this is less clear. 
}

\end{column}
\begin{column}{0.4\textwidth}

\vspace{-15mm}
<<,out.width="38mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F>>=
<<scores_plot>>
@

\vspace{-10mm}

<<,out.width="38mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F,cache=F>>=
set.seed(21)
<<mvn_plot>>
@
\end{column}
\end{columns}




\end{frame}


\begin{frame}[fragile]
\frametitle{More on covariance}
\begin{myitemize}
\item Covariance is \myemph{symmetric}: we see from the definition
\mydisplaymath{
%\hspace{-2mm}
\begin{array}{rcccl}
\vspace{4mm}
\cov(X,Y)&=&\E \Big[ \big( X-\E[X]\, \big) \, \big( Y-\E[Y] \, \big) \Big] &&
\\
&=& \E \Big[ \big( Y-\E[Y]\, \big) \, \big( X-\E[X] \, \big) \Big] &=& \cov(Y,X)
\end{array}
}
\item Also, we see from the definition that \m{\cov(X,X)=\var(X)}.
\item The \myemph{sample covariance} of \m{n} pairs of measurements \m{(x_1,y_1),\dots,(x_n,y_n)} is
\mydisplaymath{
\covSample(\vec{x},\vec{y}) = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar x)(y_i -\bar y)
}
where \m{\bar x} and \m{\bar y} are the sample means of \m{\vec{x}=(x_1,\dots,x_n)} and \m{\vec{y}=(y_1,\dots,y_n)}.

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Scaling covariance to give correlation}

\vspace{-1mm}

\begin{myitemize}

\item The standard deviation of a random variable is interpretable as its scale.
\item Variance is interpretable as the square of standard deviation
<<,eval=T>>=
var(x$midterm)
var(x$final)
cov(x$midterm,x$final)
@
\item Covariance is interpretable when scaled to give the \myemph{correlation}
\mydisplaymath{
\hspace{-2mm}
\cor(X,Y)=\frac{\cov(X,Y)}{\sqrt{\var(X)\var(Y)}} 
\hspace{20mm}
\corSample(\vec{x},\vec{y})=\frac{\covSample(\vec{x},\vec{y})}{\sqrt{\varSample(\vec{x})\varSample(\vec{y})}}
} 
<<,eval=T>>=
cor(x$midterm,x$final)
@
\end{myitemize}

\end{frame}


\begin{frame}[fragile]

<<mvn_cor_plot,eval=F,echo=F,purl=T,cache=F>>=
library(mvtnorm)
mvn <- rmvnorm(n=100,
  mean=c(X=0,Y=0),
  sigma=matrix(
    c(1,rho,rho,1),
    2,2)
)
@


\begin{columns}[T]
\begin{column}{0.45\textwidth}
<<,eval=T,cache=F>>=
rho <- 0
@
\vspace{-12mm}
<<,out.width="45mm",fig.width=3.3,fig.height=4,eval=T,echo=F,purl=F,cache=F>>=
set.seed(79)
<<mvn_cor_plot>>
plot(X~Y,data=mvn,xlab="",ylab="")
@

\vspace{-10mm}

<<,echo=T>>=
<<mvn_cor_plot>>
@
\end{column}

\begin{column}{0.45\textwidth}
<<,eval=T,cache=F>>=
rho <- -0.8
@
\vspace{-12mm}
<<,eval=T,out.width="45mm",fig.width=3.3,fig.height=4,echo=F,purl=F,cache=F>>=
set.seed(81)
<<mvn_cor_plot>>
plot(X~Y,data=mvn,xlab="",ylab="")
@

\vspace{-10mm}

<<,eval=T>>=
rho <- 0.95
@
\vspace{-12mm}
<<,eval=T,out.width="45mm",fig.width=3.3,fig.height=4,echo=F,purl=F>>=
set.seed(82)
<<mvn_cor_plot>>
plot(X~Y,data=mvn,xlab="",ylab="")
@

\end{column}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{More on interpreting correlation}

\begin{myitemize}
\item Random variables with a correlation of $\pm 1$ (or data with a sample correlation of $\pm 1$) are called \myemph{linearly dependent} or \myemph{colinear}.
\item Random variables with a correlation of 0 (or data with a sample correlation of 0) are \myemph{uncorrelated}.
\item Random variables with a covariance of 0 are also uncorrelated!

\end{myitemize}

\myquestion. Suppose two data vectors \m{\vec{x}=(x_1,\dots,x_n)} and \m{\vec{y}=(y_1,\dots,y_n)} have been \myemph{standardized}. That is, each data point has had the sample mean substracted and then been divided by the sample standard deviation. You calculate \m{\covSample(\vec{x},\vec{y})=0.8}. What is the sample correlation, \m{\corSample(\vec{x},\vec{y})}?

\answer{\vspace{25mm}}{For standardized variables, the covariance and correlation are the same.}

\end{frame}

\begin{frame}[fragile]
\frametitle{The variance of a sum}
\begin{myitemize}
\item A basic property of covariance is

\vspace{1.5mm}

\m{\mbox{(Eq. C1)}\hspace{20mm}
\var(X+Y)=\var(X)+\var(Y) + 2\, \cov(X,Y)
}

\vspace{1mm}

\item Sample covariance has the same formula,

\vspace{1.5mm}

\m{\mbox{(Eq. C2)}\hspace{20mm}
\varSample(\vec{x}+\vec{y})=\varSample(\vec{x})+\varSample(\vec{y}) + 2\, \covSample(\vec{x},\vec{y})
}

\vspace{1mm}


\item These nice formulas mean it can be easier to calculate using variances and covariances rather than standard deviations and correlations. 

\end{myitemize}

\myquestion. Rewrite (Eq. C1) to give a formula for \m{\sd(X+Y)} in terms of \m{\sd(X)}, \m{\sd(Y)} and \m{\cor(X,Y)}.

\answer{\vspace{40mm}}{
\[
[\sd(X+Y)]^2 = [\sd(X)]^2 + [\sd(Y)]^2 + \cor(X,Y)\sd(X)\sd(Y)
\]
and so
\[
\sd(X+Y) = \sqrt{[\sd(X)]^2 + [\sd(Y)]^2 + \cor(X,Y)\sd(X)\sd(Y)}
\]
}

\end{frame}

\begin{frame}[fragile]
\frametitle{More properties of covariance}
\begin{myitemize}
\item Covariance is not affected by adding constants to either variable

\vspace{2mm}

\m{\mbox{(Eq. C3)}\hspace{20mm}
\cov(X+a,Y+b)=\cov(X,Y)
}

\vspace{1mm}

\item Recall the definition \m{\cov(X,Y)=\E \big[ \big( X-\E[X]\, \big) \, \big( Y-\E[Y] \, \big) \big]}.
In words, covariance is the mean product of deviations from average. These deviations are unchanged when we add a constant to the variable.

\item Covariance scales \myemph{bilinearly} with each variable

\vspace{2mm}

\m{\mbox{(Eq. C3)}\hspace{20mm}
\cov(aX,bY)=ab \, \cov(X,Y)
}

\vspace{1mm}

\item Covariance distributes across sums

\vspace{2mm}

\m{\mbox{(Eq. C4)}\hspace{20mm}
\cov(X,Y+Z)= \cov(X,Y) + \cov(X,Z)
}

\vspace{1mm}

\item Sample covariances also have these properties. You can test them in R using bivariate normal random variables, constructed as previously using `rmvnorm()`.

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{The variance/covariance matrix of vector random variables}

\vspace{-1.5mm}

\begin{myitemize}
\item Let \m{\vec{X}=(X_1,\dots,X_p)} be a vector random variable. For any pair of elements, say \m{X_i} and \m{X_j}, we can compute the usual scalar covariance, \m{v_{ij}=\cov(X_i,X_j)}.
\item The variance/covariance matrix \m{\mat{V}=[v_{ij}]_{p\times p}} collects together all these covariances.
\mydisplaymath{
\mat{V}
=
\var(\vec{X})
=
\left[ 
\begin{array}{cccc}
\cov(X_1,X_1) & \cov(X_1,X_2) & \dots & \cov(X_1,X_p) 
\\
\cov(X_2,X_1) & \cov(X_2,X_2) &  & \cov(X_2,X_p) 
\\
\vdots &  & \ddots & \vdots
\\
\cov(X_p,X_1) & \cov(X_p,X_2) & \dots & \cov(X_p,X_p) 
\end{array}
\right]
}
\item The diagonal entries of \m{\mat{V}} are \m{v_{ii}=\cov(X_i,X_i) = \var(X_i)} for \m{i=1,\dots,p} so the variance/covariance matrix can be written as
\mydisplaymath{
\mat{V}
=
\var(\vec{X})
=
\left[ 
\begin{array}{cccc}
\var(X_1) & \cov(X_1,X_2) & \dots & \cov(X_1,X_p) 
\\
\cov(X_2,X_1) & \var(X_2) &  & \cov(X_2,X_p) 
\\
\vdots &  & \ddots & \vdots
\\
\cov(X_p,X_1) & \cov(X_p,X_2) & \dots & \var(X_p) 
\end{array}
\right]
}

\end{myitemize}

\end{frame}


\begin{frame}
\frametitle{The correlation matrix}
\begin{myitemize}
\item Covariance is harder to interpret than correlation, but easier for calculations.
\item We can put together all the correlations into a correlation matrix, using the fact that \m{\cor(X_i,X_i)=1}.
\mydisplaymath{
\cor(\vec{X})=\left[ 
\begin{array}{cccc}
1 & \cor(X_1,X_2) & \dots & \cor(X_1,X_p) 
\\
\cor(X_2,X_1) & 1 &  & \cor(X_2,X_p) 
\\
\vdots &  & \ddots & \vdots
\\
\cor(X_p,X_1) & \cor(X_p,X_2) & \dots & 1 
\end{array}
\right]
}
\item Multivariate distributions can be very complicated.
\item The variance/covariance and correlation matrices deal only with \myemph{pairwise} relationships between variables.
\item Pairwise relationships can be graphed.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The sample variance/covariance matrix}

\vspace{-1mm}

\begin{myitemize} 
\item The \myemph{sample variance/covariance matrix} places all the sample variances and covariances in a matrix.
\item Let \m{\mat{X}=[x_{ij}]_{n\times p}} be a data matrix made up of \m{p} data vectors \m{\vec{x}_1, \vec{x}_2, \dots, \vec{x_p}} each of length \m{n}.
\mydisplaymath{
\varSample(\mat{X})
=
\left[ 
\begin{array}{cccc}
\varSample(\vec{x}_1) & \covSample(\vec{x}_1,\vec{x}_2) & \dots & \covSample(\vec{x}_1,\vec{x}_p) 
\\
\covSample(\vec{x}_2,\vec{x}_1) & \varSample(\vec{x}_2) &  & \covSample(\vec{x}_2,\vec{x}_p) 
\\
\vdots &  & \ddots & \vdots
\\
\covSample(\vec{x}_p,\vec{x}_1) & \covSample(\vec{x}_p,\vec{x}_2) & \dots & \varSample(\vec{x}_p) 
\end{array}
\right]
}
\item R uses the same notation. If \code{x} is a matrix or dataframe, \code{var(x)} returns the sample variance/covariance matrix.
<<,echo=T,eval=T>>=
var(x)
@
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The sample correlation matrix}

\vspace{-1mm}

\begin{myitemize} 
\item The \myemph{sample correlation matrix} places all the sample correlations in a matrix.
\item Let \m{\mat{X}=[x_{ij}]_{n\times p}} be a data matrix made up of \m{p} data vectors \m{\vec{x}_1, \vec{x}_2, \dots, \vec{x_p}} each of length \m{n}.
\mydisplaymath{
\corSample(\mat{X})
=
\left[ 
\begin{array}{cccc}
1 & \corSample(\vec{x}_1,\vec{x}_2) & \dots & \corSample(\vec{x}_1,\vec{x}_p) 
\\
\corSample(\vec{x}_2,\vec{x}_1) & 1 &  & \corSample(\vec{x}_2,\vec{x}_p) 
\\
\vdots &  & \ddots & \vdots
\\
\corSample(\vec{x}_p,\vec{x}_1) & \corSample(\vec{x}_p,\vec{x}_2) & \dots & 1 
\end{array}
\right]
}
\item R uses the same notation. If \code{x} is a matrix or dataframe, \code{cor(x)} returns the sample correlation matrix.
<<,echo=T,eval=T>>=
cor(x)
@
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
<<scores_pairs,eval=F,echo=T>>=
pairs(x)
@

\vspace{-5mm}

<<,out.width="95mm",fig.width=5.5,fig.height=5.5,eval=T,echo=F>>=
<<scores_pairs>>
@

\end{frame}

%\end{document}


\begin{frame}[fragile]
<<mvn_pairs,eval=F,echo=T>>=
mvn <- rmvnorm(50,mean=apply(x,2,mean),sigma=var(x))
pairs(mvn)
@

\vspace{-5mm}

<<,out.width="90mm",fig.width=5.5,fig.height=5.5,eval=T,echo=F>>=
set.seed(70)
<<mvn_pairs>>
@

\end{frame}

\begin{frame}[fragile]
\myquestion. From looking at the scatterplots, what are the strengths and weaknesses of a multivariate normal model for test scores in this course?

\answer{\vspace{30mm}}{
`hw` does not have a bell-shaped distribution. It has most of the measurements bunched around 95\% and a long left tail. Likely, the homework scores were dominated by a few rare events concerning whether the work was completed and turned in, with scores being close to 100% on completed homework. The quiz shows some of this pattern, but to a lower extent.
}

\myquestion. To what extent is it appropriate to summarize the data by the mean and variance/covariance matrix (or correlation matrix) when the normal approximation is dubious?

\answer{\vspace{30mm}}{The sample mean, variance and covariance (or sample mean, standard deviation and correlation) are useful for non-normal data. They tell you everything needed to describe a normal distribution but for non-normal data there may be aspects not revealed by these numbers. Plot the data! 
}

\end{frame}

\begin{frame}[fragile]
%\frametitle{Variance/covariance of a linear combination}
\frametitle{Linear combinations}

\vspace{-1.5mm}

\begin{myitemize}
\item Let \m{\vec{X}=(X_1,\dots,X_p)} be a vector random variable with mean vector \m{\vec{\mu}=(\mu_1,\dots,\mu_p)} and \m{p\times p} variance/covariance matrix \m{\mat{V}}.
\item Let \m{\mat{X}} be a \m{n\times p} data matrix.
\item Let \m{\mat{A}} be a \m{q\times p} matrix.
\item \m{\vec{Z}=\mat{A}\vec{X}} is a collection of \m{q} linear combinations of the \m{p} random variables in the vector \m{\vec{X}}, viewed as a \myemph{column} vector.
\item \m{\mat{Z}=\mat{X}\mat{A}^\transpose} is an \m{n\times q} collection of linear combinations of the \m{p} data points in each \myemph{row} of \m{\mat{X}}.
\item Mental gymnastics are required: vectors are often interpreted as \myemph{column vectors} (e.g., \m{p\times 1} matrices) but the vector of measurements for each unit is a \myemph{row vector} when considered as a row of an \m{n\times p} data matrix. 

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Variables of length $p$ as column vectors or row vectors}

\myquestion. How would you construct a simulated data matrix \m{\mat{Z}_\mathrm{sim}} from \m{n} realizations \m{\vec{Z}_1,\dots,\vec{Z}_n} of the random column vector \m{\vec{Z}=\mat{A}\vec{X}}? 

Hint: You are expected to write a matrix constructing \m{\mat{Z}_\mathrm{sim}} by stacking together 
\m{\vec{Z}_1,\dots,\vec{Z}_n}. Be careful with transposes and keep track of dimensions. Recall that \m{\mat{Z_\mathrm{sim}}} should be \m{n\times p}.

\answer{\vspace{50mm}}{
$\vec{Z}$ is $q\times 1$. Each realization should correspond to a row of an $n\times q$ data matrix comparable to $\mat{Z}$, so 
\[ 
\vspace{-3mm}
\mat{Z}_\mathrm{sim} = 
\left[
\begin{array}{c}
\vec{Z}_1^\transpose \\
\vec{Z}_2^\transpose \\
\vdots \\
\vec{Z}_n^\transpose
\end{array}
\right]
\vspace{-3mm}
\]
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Expectation and variance of linear combinations}

\vspace{-1.5mm}

\begin{myitemize}
\item The expectation of a vector random variable is the vector of expectations of each element.
If \m{\vec{X}=(X_1,\dots,X_n)} then
\mydisplaymath{
\E[\vec{X}]=\big( \, \E[X_1],\E[X_2],\dots,\E[X_n]\, \big)
}
\item The expectation of a sum is the sum of the expectations.

\vspace{2mm}

\centerline{
  \framebox{
    \m{ 
       \E[X+Y]=\E[X]+\E[Y]
    }
  }
}

\vspace{1mm}

\item This formula extends to any linear combination of \m{n} random variables. If \m{\vec{Z}=\mat{A}\vec{X}} then \m{\E[\vec{Z}]=\mat{A}\, \E[\vec{X}]}. In other words,

\vspace{2mm}

\centerline{
  \framebox{
    \m{
    \E[\mat{A}\, \vec{X}]= \mat{A}\, \E[\vec{X}]
    }
  }
}

\vspace{1mm}

\item There is a useful matrix variance/covariance formula for a linear combination, which also works for sample variance/covariance. 

\vspace{2mm}

%\mydisplaymath{
\framebox{
  \m{
    \var(\mat{A}\, \vec{X}) = \mat{A}\, \var(\vec{X})\, \mat{A}^\transpose 
  }
}
\hspace{20mm}
\framebox{
  \m{
    \varSample(\mat{X}\, \mat{A}^\transpose) = \mat{A}\, \varSample(\mat{X})\, \mat{A}^\transpose
  }
}
\end{myitemize}

\end{frame}

\begin{frame}
\frametitle{Exercises with the matrix variance/covariance formula}

\myquestion. Add dimensions to each quantity in the equations \m{ \var(\mat{A}\, \vec{X}) = \mat{A}\, \var(\vec{X})\, \mat{A}^\transpose } and \m{\varSample(\mat{X}\, \mat{A}^\transpose) = \mat{A}\, \varSample(\mat{X})\, \mat{A}^\transpose}.

\answer{\vspace{20mm}}{
\vspace{-5mm}
\[
\underbrace{
\var( 
  \underset{q\times p }{\mat{A}}\,
  \underset{p\times 1}{\vec{X}}
)
}_{q\times q}
= 
\underset{q\times p}{\mat{A}}
\, 
\underbrace{
\var(
  \underset{p\times 1}{\vec{X}}
)
}_{p\times p}
\, 
\underset{p\times q}{\mat{A}^\transpose}
\hspace{10mm}
\underbrace{
\varSample(
  \underset{n\times p}{\mat{X}} \,
  \underset{p\times q}{\mat{A}^\transpose}
)
}_{q\times q}
= 
\underset{q\times p}{\mat{A}}
\, 
\underbrace{
\varSample(
  \underset{n\times p}{\mat{X}}
)
}_{p\times p}
 \, 
\underset{p\times q}{\mat{A}^\transpose}
\]
}


\myquestion.  Let \m{\mat{A}=[1 \dots 1]} be the \m{1\times p} row vector of ones. Let \m{\vec{X}=(X_1,\dots,X_p)} be a vector random variable with variance/covariance matrix \m{\mat{V}=[V_{ij}]_{p\times p}}. Evaluate the variance/covariance formula in this case. Hence, find
\m{\var(\bar X)} where \m{\bar X = (1/p)\sum_{i=1}^p X_i}.

\answer{\vspace{50mm}}{todo}

\end{frame}

\begin{frame}[fragile]
\frametitle{Testing the variance/covariance formula}

\vspace{-1mm}

\begin{myitemize}
\item Suppose that the overall course score is weighted 40\% on the final and 20\% on each of the miterm, homework and quiz.
\item We can find the sample variance of the overall score two different ways. 
\end{myitemize}

\vspace{1mm}

(i) Directly computing the overall score for each student.
<<>>= 
weights <- c(final=0.4,quiz=0.2,hw=0.2,midterm=0.2)
overall <- as.matrix(x) %*% weights
var(overall)
@
(ii) Using \m{\varSample(\mat{X}\, \mat{A}^\transpose) = \mat{A}\, \varSample(\mat{X})\, \mat{A}^\transpose}.
<<>>=
weights %*% var(x) %*% weights
@

\vspace{-1mm}

\begin{myitemize}
\item R interprets the vector `weights` as a row or column vector as necessary.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Independence}
\begin{myitemize}
\item Two events \m{E} and \m{F} are \myemph{independent} if
\mydisplaymath{
\prob(\mbox{$E$ and $F$})= \prob(E) \times \prob(F)
}
\end{myitemize}
\WorkedExample. Suppose we have a red die and a blue die. They are idea fair dice, so the values should be independent. What is the chance they both show a six? 

(a) Using the definition of independence.

\answer{\vspace{20mm}}{
$\prob\big(\{\mbox{red is 6}\} \mbox{ and } \{\mbox{blue is 6}\} \big)=
\prob(\mbox{red is 6}) \, prob(\mbox{blue is 6}) = 1/6 \times 1/6 = 1/36$
}

(b) By considering equally likely outcomes, without using the definition.

\answer{\vspace{20mm}}{There are $6\times 6=36$ possible outcomes $(\mbox{red die},\mbox{blue die})$. By symmetry, all are equally likely, so the outcome $(6,6)$ has probability $1/36$.}

\begin{myitemize}
\item The multiplication rule agrees with an intuitive idea of independence.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Independence of random variables}

\vspace{-2mm}

\begin{myitemize}
\item \m{X} and \m{Y} are \myemph{independent random variables} if, for any intervals \m{[a,b]} and \m{[c,d]},

\vspace{-5mm}

\mydisplaymath{
\prob(\mbox{$a<X<b$ and $c<Y<d$})= \prob(a<X<b) \times \prob(c<Y<d)
}

\vspace{-2mm}

\item This definition extends to vector random variables. \m{\vec{X}=(X_1,\dots,X_n)} is a \myemph{vector of independent random variables} if for any collection of intervals \m{[a_i,b_i]}, \m{1\le i\le n},
\mydisplaymath{
\hspace{-3mm}
\prob(a_1{<}X_1<{b}_1, \dots, a_n{<}X_n{<}b_n)= \prob(a_1{<}X_1{<}b_1) \times\dots\times \prob(a_n{<}X_n{<}b_n)
}

\vspace{-1mm}

\item \m{\vec{X}=(X_1,\dots,X_n)} is a \myemph{vector of independent identically distributed (iid) random variables} if, in addition, each element of \m{\vec{X}} has the same distribution.
\item 
``\m{X_1,\dots,X_n} are \m{n} random variables with the \m{\normal(\mu,\sigma)} distribution''

\centerline{is written more formally as}

\centerline{``Let \m{X_1,\dots,X_n \sim \iid \;  \normal(\mu,\sigma)}.''}
\end{myitemize}

\vspace{2mm}

\end{frame}

\begin{frame}[fragile]
\frametitle{Independent vs uncorrelated}
\begin{myitemize}
\item If \m{X} and \m{Y} are independent they are uncorrelated. 
\item The converse is not necessarily true.
\item \myemph{For normal random variables, the converse is true}.
\item If \m{X} and \m{Y} are bivariate normal random variables, and \m{\cov(X,Y)=0}, then \m{X} and \m{Y} are independent.
\item The following slide demonstrated the possibility of being uncorrelated but not independent (for non-normal random variables).
\item If the scatter plot of two variables looks normal and their sample correlation is small, the variables are appropriately modeled as independent.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Zero correlation with and without independence}

\vspace{-2mm}

\begin{columns}[T]
\begin{column}{0.45\textwidth}
<<,echo=F>>=
set.seed(368) 
@

<<normal_uncorrelated,echo=T,eval=T>>=
x <- rnorm(20)
y <- rnorm(20)
@

\vspace{-3mm}

<<>>=
cor(x,y)
@

\vspace{-3mm}

<<plot_normal_uncorrelated,echo=T,eval=F>>=
plot(x,y)
@

\vspace{-10mm}

<<,out.width="50mm",fig.width=3.5,fig.height=3.5,purl=F,echo=F>>=
<<plot_normal_uncorrelated>>
@

\end{column}
\begin{column}{0.45\textwidth}
<<quadratic_uncorrelated,eval=T,echo=T>>=
x <- seq(-2,2,length=20)
y <- x^2
@

\vspace{-3mm}

<<>>=
cor(x,y)
@

\vspace{-3mm}

<<plot_quadratic_uncorrelated,eval=F,echo=T>>=
plot(x,y)
@

\vspace{-10mm}

<<,out.width="50mm",fig.width=3.5,fig.height=3.5,purl=F,echo=F>>=
<<plot_quadratic_uncorrelated>>
@

\end{column}
\end{columns}
\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{The measurement error model}
\begin{myitemize}

\item Let \m{\vect{\epsilon}=(\epsilon_1,\epsilon_2,\dots,\epsilon_n)} be a vector consisting of \m{n} independent normal random variables, each with mean zero and variance \m{\sigma^2}.
\item This is a common model for \myemph{measurement error} on \m{n} measurements.

\item The mean vector and variance/covariance matrix are
\vspace{-2mm}

\mydisplaymath{\E[\vect{\epsilon}]=\vect{0},\hspace{2cm} \var(\vect{\epsilon})=\sigma^2 \mat{I}}

%\vspace{-2mm}

where \m{\vect{0}=(0,\dots,0)} and \m{\mat{I}} is the \m{n\times n} identity matrix.


%\item The off-diagonal entries of \m{\var(\vect{\epsilon})} are zero since \m{\cov(\epsilon_i,\epsilon_j)=0} for \m{i\neq j}.

\item For the measurement error model, we break our usual rule of using upper case letters for random variables.

\item We can write \m{\vect{\epsilon}\sim\MVN(\vect{0},\sigma^2\mat{I})} or \m{\vect{\epsilon}\sim\iid\; \normal(0,\sigma)}.

\item \m{\vec{\epsilon}} models \myemph{unbiased} measurements (meaning \m{\E[\epsilon_i]=0}) subject to \myemph{additive Gaussian error}.

\end{myitemize}

\vspace{1mm}

Example: 5 repeated measurements \m{x_1,\dots,x_5} of the speed of light could be modeled as \m{X_i = \mu + \epsilon_i} for \m{i=1,\dots,5}, where \m{\mu} is the unknown true value of this quantity.

\end{frame}

\begin{frame}[fragile]
\frametitle{A probability model for the linear model}

\vspace{-2mm}

\begin{myitemize}
\item First recall the sample version of the linear model, which is

\vspace{-3mm}

\mydisplaymath{
 \vect{y} = \mat{X}\, \vect{b} + \vect{e},
}

\vspace{1mm}

where \m{\vect{y}} is the measured response, 
\m{\mat{X}} is an \m{n\times p}  matrix of explanatory variables, \m{\vect{b}} is chosen by least squares, and \m{\vect{e}} is the vector of residuals.

\item We want to build a random vector \m{\vect{Y}} that provides a probability model for the data \m{\vect{y}}. We write this as

\vspace{2mm}

\centerline{
  \framebox{
    \m{
       \vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}
       \rule[-2mm]{0mm}{7mm}
    }
  }
}
\vspace{2mm}

where \m{\mat{X}} is the explanatory matrix, \m{\vect{\beta}=(\beta_1,\dots,\beta_p)} is an unknown coefficient vector (we don't know the true population coefficient!) and \m{\vect{\epsilon}} is Gaussian measurement error with \m{\E[\vect{\epsilon}]=\vect{0}} and \m{\var(\vect{\epsilon})=\sigma^2\mat{I}}.

\item Our probability model  asserts that the process which generated the response data \m{\vect{y}} was like drawing a random vector \m{\vect{Y}} constructed using a random measurement error model with known matrix \m{\mat{X}} for some fixed but unknown value of \m{\vect{\beta}}.

\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{A digression on ``useful'' models}

``Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law $PV = RT$ relating pressure $P$, volume $V$ and temperature $T$ of an \emph{ideal} gas via a constant $R$ is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules.
For such a model there is no need to ask the question `Is the model true?'. If \emph{truth} is to be the \emph{whole truth} the answer must be \emph{No}. The only question of interest is `Is the model illuminating and useful.' '' (Box, 1978)

``\myemph{Essentially, all models are wrong, but some are useful.}'' \\ (Box and Draper, 1987)

\begin{myitemize}
\item Perhaps the most useful statistical model ever is \m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}}.
\item Anything so widely used is also widely abused. Our task is to understand \m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}} so that we can use it wisely.
\end{myitemize}

\end{frame}

\begin{frame}
\frametitle{Expectation and variance/covariance of $\vec{Y}$}
\begin{myitemize}
\vspace{-1.5mm}

\item Recall the linear model \m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}} 
 where \m{\vec{\epsilon}\sim \MVN(\vect{0},\sigma^2\mat{I})}.
\end{myitemize}

\myquestion. What is the expected value, \m{\E[\vect{Y}]}?

\answer{\vspace{30mm}}{It may be helpful to write this in subscript form. For each $i$, 
$\E[Y_i]=\E[\sum_{j=1}^p x_{ij}\beta_j + \epsilon_i] = \sum_{j=1}^px_{ij}\beta_i$. So,
$\E[\vec{Y}]=\mat{X}\vec{\beta}$. We can also just do matrix calculations.}

\myquestion. What is the variance/covariance matrix, \m{\var(\vect{Y})}?
 
\answer{\vspace{35mm}}{Using subscript form,
$\cov(Y_i,Y_j)=\cov\left(\sum_{k=1}^p x_{ik}\beta_k + \epsilon_i, \sum_{\ell=1}^p x_{j\ell}\beta_\ell + \epsilon_j \right) = \cov(\epsilon_i,\epsilon_j)$. Since $\vec{\epsilon} \sim\MVN(\vec{0},\sigma^2\mat{I})$, this gives $\cov(\vec{Y})=\sigma^2\mat{I}$.
}

\end{frame}

\begin{frame}
\frametitle{Expectation of the least squares coefficient}

\WorkedExample.
Let \m{\vect{\hat\beta}= \big( \mat{X}^\transpose \mat{X} \big)^{-1}\, \mat{X}^\transpose \vect{Y}}. This is the probability model for the sample least squares coefficient \m{\vect{b}= \big( \mat{X}^\transpose \mat{X} \big)^{-1}\, \mat{X}^\transpose \vect{y}}.
Use linearity to calculate \m{\E[\vect{\hat\beta}]}.

\vspace{4mm}

{\bf Solution}:
\mydisplaymath{
\begin{array}{rcl}
\E[\vect{\hat\beta}] &=& \E\big[ \, \big( \mat{X}^\transpose \mat{X} \big)^{-1}\, \mat{X}^\transpose \vect{Y} \, \big]
\\
&=& \rule[-2mm]{0mm}{7mm}
 \big( \mat{X}^\transpose \mat{X} \big)^{-1}\, \mat{X}^\transpose \, \E\big[ \vect{Y} \big]
\\
&=& \rule[-2mm]{0mm}{7mm}
 \big( \mat{X}^\transpose \mat{X} \big)^{-1}\, \mat{X}^\transpose \mat{X}\, \vec{\beta}
\\
&=& \rule[-2mm]{0mm}{7mm}
\vec{\beta}
\end{array}
}

\vspace{3mm}

\begin{myitemize}
\item Interpretation: If the data \m{\vect{y}} are well modeled as a draw from the probability model 
\m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}}, then the least squares estimate \m{\vect{b}} is well modeled by a random vector with mean \m{\vect{\beta}}.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Variance/covariance matrix of the least squares coefficients}

\myquestion. Consider the linear model \m{\vect{Y}=\mat{X}\vect{\beta}+\vect{\epsilon}} with \m{\E[\vect{\epsilon}]=\vect{0}} and \m{\var(\vect{\epsilon})=\sigma^2\mat{I}}.
Use the formula \m{\var(\mat{A}\vec{Y})=\mat{A}\var(\vec{Y})\mat{A}^\transpose} to find \m{\var(\vec{\hat\beta})}. Hint: what should be our choice of \m{\mat{A}} so that \m{\vec{\hat\beta}= \mat{A}\vec{Y}}?

\answer{\vspace{80mm}}{
Recall
\[
\vect{\hat\beta}= \big( \mat{X}^\transpose \mat{X} \big)^{-1}\, \mat{X}^\transpose \vect{Y}.
\]
Then, the variance/covariance formula gives (after some lines of algebra)
\framebox{
  $\var(\vect{\hat\beta}) =\sigma^2 \big( \mat{X}^\transpose \mat{X} \big)^{-1}$
  \rule[-3mm]{0mm}{9mm}
  }
}

\vspace{40mm}


\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Standard errors for coefficients in the linear model}

\begin{myitemize}
\item The formula  \m{\var(\vect{\hat\beta})
  =\sigma^2 \big( \mat{X}^\transpose \mat{X} \big)^{-1}
 } needs extra work to be useful for data analysis. 
\item In practice, we know the model matrix \m{\mat{X}} but we don't know the measurement standard deviation \m{\sigma}.
\item An estimate of the measurement standard deviation is the sample standard deviation of the residuals. 
\item 
For  \m{\vect{y} = \mat{X}\, \vect{b} + \vect{e}} with \m{\mat{X}} being \m{n\times p}, an estimate of \m{\sigma} is
\framebox{
\m{ 
 s = 
 \sqrt{\frac{1}{n-p} \sum_{i=1}^n \big(y_i - \hat y_i\big)^2}
 =
 \sqrt{\frac{1}{n-p} \sum_{i=1}^n \big(y_i - [\mat{X}\vect{b}]_i\big)^2}
}
\rule[-3mm]{0mm}{9mm}
}
\item We will discuss later why we choose to divide by \m{n-p}.
\item The \myemph{standard error} of \m{b_k} for \m{k=1,\dots,p} is 
\framebox{
\m{ \SE(b_k) = s \, \sqrt{ \big[\big(\mat{X}^\transpose \mat{X} \big)^{-1} \big]_{kk} }
}
\rule[-3mm]{0mm}{9mm}
} which is an estimate of \m{\sqrt{\big[\var(\vect{\hat\beta})\big]_{kk}}}.
\item 
%\m{SE(b_k)} is an estimate of \m{\sqrt{\big[\var(\vect{\hat\beta})\big]_{kk}}}. It is used by 
These standard errors are calculated by \code{lm()} in R. 
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
<<reconstruct_variables,echo=F>>=
L <- read.table(file="life_expectancy.txt",header=TRUE)
L_fit <- lm(Total~Year,data=L)
L_detrended <- L_fit$residuals
U <- read.table(file="unemployment.csv",sep=",",header=TRUE)
U_annual <- apply(U[,2:13],1,mean)
U_detrended <- lm(U_annual~U$Year)$residuals
L_detrended <- subset(L_detrended,L$Year %in% U$Year)
@


<<detrended_lm>>= 
lm1 <- lm(L_detrended~U_detrended) ; summary(lm1)
@


\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{How does R obtain linear model standard errors?}

\vspace{-1.5mm}

\begin{myitemize}
\item The previous slide shows output from our analysis of unemployment and mortality from Chapter~1.
\item Let's first extract the estimates and their standard errors from R, a good step toward reproducible data analysis.
\end{myitemize}

\vspace{-1mm}

<<summary>>=
names(summary(lm1))
summary(lm1)$coefficients
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Extracting the design matrix}

<<model_matrix>>=
X <- model.matrix(lm1)
head(X)
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Computing the standard errors directly}

\vspace{-1mm}

<<se>>=
s <- sqrt(sum(resid(lm1)^2)/(nrow(X)-ncol(X))) ; s
V <- s^2 * solve(t(X)%*%X)
sqrt(diag(V))
@
\begin{myitemize}
\item This matches the standard errors generated by \code{lm()}.
\end{myitemize}
<<>>=
summary(lm1)$coefficients
@

\end{frame}

\begin{frame}[fragile]
\frametitle{Extracting the coefficient variance/covariance matrix}

\begin{myitemize}
\item The fitted \code{lm} object in R stores the estimated variance/covariance matrix for the coefficients in the output of \code{summary()}.
\end{myitemize}
\vspace{-1mm}

<<var_from_summary>>=
s <- summary(lm1)$sigma
XX <- summary(lm1)$cov.unscaled
s^2 * XX
@

\begin{myitemize}
\item This matches what we get from calculating \m{s^2 \big( \mat{X}^\transpose \mat{X} \big)^{-1}} directly.
\end{myitemize}

<<var_from_direct_calculation>>=
X <- model.matrix(lm1)
sum(resid(lm1)^2)/(nrow(X)-ncol(X)) * solve(t(X)%*%X)
@

\end{frame}

\end{document}

