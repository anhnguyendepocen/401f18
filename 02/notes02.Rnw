%\documentclass[handout]{beamer}
\documentclass{beamer}

\input{../header.tex}
\newcommand\CHAPTER{2}
% \newcommand\answer[2]{\textcolor{blue}{#2}} % to show answers
 \newcommand\answer[2]{{\color{blue}{#2}}} % to show answers
% \newcommand\answer[2]{#1} % to show blank space
<<R_answer,echo=F,purl=F>>=
# ANS = TRUE
 ANS=FALSE
@

\newcommand\Li{\mathrm{(L1)}}
\newcommand\Lii{\mathrm{(L2)}}
\newcommand\Liiia{\mathrm{(L3.1)}}
\newcommand\Liiib{\mathrm{(L3.2)}}

% used for \utilde in alternative vector notation
\usepackage{undertilde}


\begin{document}

% knitr set up
<<knitr_opts,echo=F,cache=F,purl=F>>=
library(knitr)
opts_chunk$set(
#  cache=FALSE,
  cache=TRUE,
  eval=TRUE,
  include=TRUE,
  echo=TRUE,
  purl=TRUE,
  cache.path=paste0("tmp/cache"),
  dev='png',
  dev.args=list(bg='transparent'),
  dpi=300,
  error=FALSE,
  fig.pos="h!",
  fig.align='center',
  fig.height=4,fig.width=6.83,
  fig.lp="fig:",
  fig.path=paste0("tmp/figure"),
  fig.show='asis',
  highlight=TRUE,
  message=FALSE,
  progress=TRUE,
  prompt=FALSE,
#  results='asis',
  results="markup",
  size='small',
  strip.white=TRUE,
  tidy=FALSE,
  warning=FALSE
#  comment=NA # to remove ## on output
)
options(width = 60) # number of characters in R output before wrapping
@

% other set up
<<setup,echo=F,results=F,cache=F>>=
library(broman) # used for myround 
@

<<read_e0,echo=F,results=F>>=
L <- read.table(file="life_expectancy.txt",header=TRUE)
e0 <- L$Total
n <- length(e0)
@

\begin{frame}
\frametitle{\CHAPTER. Linear algebra for applied statistics}

\vspace{-2mm}

\begin{myitemize}
\item Linear algebra is the math of vectors and matrices.
\item In statistics, the main purpose of linear algebra is to organize data and write down the manipulations we want to do to them.
\item A \myemph{vector} of length \mymath{n} is also called an \mymath{n}-\myemph{tuple} or a \myemph{sequence} of length \mymath{n}. 
\item We can suppose that each data point is a \myemph{real number}. We write \mymath{\Rspace} for the set of real numbers, and \mymath{\Rspace^n} for the set of vectors of \mymath{n} real numbers.
\item Write the US life expectancy at birth for \Sexpr{L$Year[n-4]} to \Sexpr{L$Year[n]} as
\mymath{\vec{y}=(y_1,y_2,y_3,y_4,y_5)=( \Sexpr{myround(e0[n],1)},\Sexpr{myround(e0[n-1])},\Sexpr{myround(e0[n-2],1)},\Sexpr{myround(e0[n-3],1)},\Sexpr{myround(e0[n-4],1)} )}. We see \mymath{\vec{y}\in\Rspace^5}. 
\item A numerical dataset with \mymath{n} datapoints is a vector in \mymath{\Rspace^n}.
\item Qualitative data can be written as a vector in \mymath{\Rspace^n} by assigning a number for each qualitative level.
\item We use bold font for vectors, and italic font for \myemph{elements} of the vector. 
\end{myitemize}
 

%\vspace{-2mm}
\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{More perspectives on vectors}


{\myquestion}. 
You may or may not have seen vectors in other contexts. In physics, a vector is a quantity with magnitude and direction. How does that fit in with our definition?

\answer{\vspace{25mm}}{This physics definition is a polar coordinate representation of a vector in \m{\Rspace^2} and \m{\Rspace^3}. Our definition is a Cartesian coordinate representation.

\vspace{5mm}}

{\myquestion}. 
How can I write a boldface \m{\vec{x}} in handwriting?

\answer{\vspace{25mm}}{
An underscore, \m{\utilde{x}}, is conventional handwriting to represent bold font.
In physics and mathematics, vectors are sometimes written as \m{\stackrel{\to}{x}}, but we will not do that here.
}

\end{frame}


\begin{frame}[fragile]
\frametitle{Adding vectors and multiplying by a scalar}
\begin{myitemize}
\item
For a dataset, the \myemph{index} \m{i} of the element \m{y_i} of the vector \m{y} might correspond to a measurement on the \m{i}th member of a population, the outcome of the \m{i}th group in an experiment, or the \m{i}th observation out of a sequence of observations on a system.
Recall \m{i} is called an \myemph{observational unit}, or just \myemph{unit}.
\item
We might want to add two quantities \m{u_i} and \m{v_i} for unit \m{i}.
\item
Using vector notation, if \m{\vec{u}=(u_1,u_2,\dots,u_n)}, \m{\vec{v}=(v_1,v_2,\dots,v_n)} and \m{\vec{y}=(y_1,y_2,\dots,y_n)} we define the \myemph{vector sum} \m{\vec{y}=\vec{u}+\vec{v}} to be the \myemph{elementwise sum} \m{y_i=u_i+v_i}, adding up the corresponding elements for each unit.
\item 
We might also want to rescale each element by the same factor. 
To change a measurement \m{y_i} in inches to a new measurement \m{z_i} in mm, we rescale with the \myemph{scalar} \m{\alpha=25.4}. 
We want \m{z_i=\alpha y_i} for each \m{i}. 
This is written in vector notation as \myemph{multplication of a vector by a scalar}, \m{\vec{z}=\alpha \vec{y}}.
\end{myitemize} 

\end{frame} 

\begin{frame}[fragile]
\frametitle{An example of vector addition and scalar multiplication}

%{\bf Worked example}. 
An ecologist measures the pH of ten Michigan lakes at two points in the summer. Set up vector notation to describe her data algebraically. Write a vector calculation giving the average pH in each lake.

\answer{\vspace{90mm}}{
\begin{myitemize}
\item Let \mymath{x_i} be the first pH measurement in lake \mymath{i}, for \mymath{i\in\{1,2,\dots,10\}}.
\item 
Write \mymath{\vec{x}=(x_1,\dots,x_{10})} for the vector of the first pH measurement in each of the 10 lakes.
\item
 Let \mymath{\vec{y}=(y_1,\dots,y_{10})} be the vector of second measurements. 
\item
Let \mymath{\vec{\mu}=(\mu_1,\dots,\mu_{10})} be the average pH for each of the 10 lakes.
\item
For each lake \mymath{i}, the mean is \mymath{\mu_i = \frac{1}{2}(x_i+y_i)}. 
\item In vector notation,
this is 
\mydisplaymath{
\vec{\mu} = \frac{1}{2}(\vec{x}+\vec{y})}.
\end{myitemize}
}

\end{frame}

\begin{frame}[fragile]
\frametitle{Vectors and scalars in R}

\vspace{-2mm}

\begin{myitemize}
\item
We have seen in Chapter 1 that R has vectors.
An R vector of length 1 is a scalar, and we have seen that R follows the usual mathematical rules of vector addition and multiplication by a scalar.
\item R also allows adding a scalar to a vector
\end{myitemize}
\begin{columns}[T]
\begin{column}{0.3\textwidth}
<<>>=
x <- c(1,2,3)
@
\end{column}
\begin{column}{0.3\textwidth}
<<>>=
x+2 
@
\end{column}
\begin{column}{0.3\textwidth}
\end{column}
\end{columns}
\begin{myitemize}
\item Mathematically, adding scalars to vectors is \myemph{not allowed}. Instead, we define the \myemph{vector of ones}, \m{\vec{1}=(1,1,\dots,1)}, and write \m{\vec{x}+ 2\times \vec{1}}.
\end{myitemize}


\myquestion. Why does R choose to break the usual rules of mathematics here?

\answer{\vspace{25mm}}{Convenience. It is quite common to want to add a fixed amount to each data point, so R makes this easy.}

\end{frame}

%\newcommand{\Amat}{A}
%\newcommand{\Aelement}{a}

\begin{frame}[fragile]
\frametitle{Matrices}
\begin{myitemize}
\item Matrices let us store and manipulate \m{p} quantitites for each of \m{n} units.
\item An \m{n\times p} matrix \m{\mat{A}} is a numerical array with \m{n} rows and \m{p} columns,
\altdisplaymath{
\mat{A} = \mymatrix{a}{n}{p}
}
\item
Blackboard bold capital letters, \m{\mat{A}}, \m{\mat{B}}, \m{\mat{X}}, \m{\mat{Z}}, etc, denote matrices. 
\item
We are reserving plain capital letters for random variables.
\item We say \m{\mat{A}=[a_{ij}]_{n\times p}} as an abbreviation for writing the full \m{n\times p} matrix.
\item We write \m{\underset{n\times p}{\mat{A}}} to emphasize the dimension. 
\item Keeping track of matrix dimensions can be helpful!
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Matrix addition}

\vspace{-1mm}

\begin{myitemize}
\item matrices are added elementwise.
\item We can only add matrices with the same dimensions.
\item Let \m{\mat{A}=[a_{ij}]_{n\times p}} and \m{\mat{B}=[b_{ij}]_{n\times p}}. 
\item Since both \m{\mat{A}} and \m{\mat{B}} are \m{n\times p} we can add them

\vspace{2mm}

\m{
\left[
\begin{array}{cccc}
\textcolor{red}{a_{11}} & a_{12} & \dots & a_{1p} \\
a_{21} & a_{22} & \dots & a_{2p} \\
\vdots &        &       & \vdots  \\
a_{n1} & a_{n2} & \dots & a_{np}
\end{array}
\right]
+
\left[
\begin{array}{cccc}
\textcolor{red}{b_{11}} & b_{12} & \dots & b_{1p} \\
b_{21} & b_{22} & \dots & b_{2p} \\
\vdots &        &       & \vdots  \\
b_{n1} & b_{n2} & \dots & b_{np}
\end{array}
\right]
}

\vspace{2mm}

\m{ \hspace{20mm}=
\left[
\begin{array}{cccc}
\textcolor{red}{a_{11}+b_{11}} & a_{12}+b_{12} & \dots & a_{1p}+b_{1p} \\
a_{21}+b_{22} & a_{22}+b_{22} & \dots & a_{2p}+b_{2p} \\
\vdots &        &       & \vdots  \\
a_{n1}+b_{n1} & a_{n2}+b_{n2} & \dots & a_{np}+b_{np}
\end{array}
\right]
}

\vspace{1mm}

\item Focus on one element (say, the top left element in red) and then check that the pattern repeats.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Matrix addition in R}
\begin{myitemize}
\item R agrees with our mathematical definition of matrix addition.
\item If \code{A} and \code{B} are two R matrices of matching dimension, then
<<,eval=F,purl=F>>=
C <- A + B
@ 
gives a matrix with the elementwise sums.
\item If the dimensions of \code{A} and \code{B} don't match, R gives an error message.
\item Curiously, R does allow adding a vector to a matrix using the recycling rule. This is \myemph{not allowed} in math.
<<matrix plus vector>>=
A <- matrix(1:6,2,3)
A
A+c(0,10)
@
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Matrix multiplication}
\begin{myitemize}
\item The rule for matrix multiplication seems strange at first. We'll see why it is useful.
\item The \m{(i,j)} entry of the product \m{\mat{A}\, \mat{B}} comes from multiplying each element in row i of \m{\mat{A}} with the corresponding element in column j of \m{\mat{B}} and adding up these terms.
\item For this rule to work, the number of columns of \m{\mat{A}} must match the number of rows of \m{\mat{B}}. 
\item We start with the \m{2\times 2} case, where \m{\mat{A}=[a_{ij}]_{2\times 2}} and \m{\mat{B}=[b_{ij}]_{2\times 2}}.
\mydisplaymath{
\left[
\begin{array}{cc}
\textcolor{red}{a_{11}} & \textcolor{red}{a_{12}} \\
a_{21} & a_{22} \\
\end{array}
\right]
\hspace{1mm}
\left[
\begin{array}{cc}
b_{11} & \textcolor{red}{b_{12}} \\
b_{21} & \textcolor{red}{b_{22}} \\
\end{array}
\right]
=
\left[
\begin{array}{cc}
a_{11}b_{11}+a_{12}b_{21} & \textcolor{red}{a_{11}b_{12}+a_{12}b_{22}} \\
a_{21}b_{11}+a_{22}b_{21} & a_{21}b_{12}+a_{22}b_{22} \\
\end{array}
\right]
}
\item In red: notice how the \m{(1,2)} entry of \m{\mat{A}\, \mat{B}} comes from \myemph{sliding} row 1 of \m{\mat{A}} down column 2 of \m{\mat{B}} and adding corresponding terms.

\end{myitemize} 

\end{frame}


\begin{frame}[fragile]
\frametitle{Let's practice multiplying $2\times 2$ matrices}
\myquestion. Evaluate the matrix product
\mydisplaymath{
\left[
\begin{array}{cc}
{2} & -1 \\
1 & 3 \\
\end{array}
\right]
\hspace{1mm}
\left[
\begin{array}{cc}
1 & 4 \\
-2 & 0 \\
\end{array}
\right]
}

\answer{\vspace{80mm}}{
Answer:
\mydisplaymath{
\left[
\begin{array}{cc}
4 & 8 \\
-5 & 4 \\
\end{array}
\right]
}
\vspace{30mm}

}

\end{frame}

\begin{frame}
\frametitle{The dimension of a matrix product}
\begin{myitemize}
\item We can multiply \m{\underset{n\times p}{\mat{A}} \hspace{2mm} \underset{p\times q}{\mat{B}} } and the result is an \m{n\times q} matrix.
\end{myitemize}
\myquestion. What do you think is the dimension of the triple product 
\mydisplaymath{
\underset{n\times \textcolor{red}{p}}{\mat{A}}
\hspace{2mm}
\underset{\textcolor{red}{p}\times \textcolor{cyan}{q}}{\mat{B}} 
\hspace{2mm}
\underset{\textcolor{cyan}{q}\times r}{\mat{C}}
}

\answer{\vspace{15mm}}{\vspace{15mm}}

\begin{myitemize}
\item The rule is: when multiplying matrices, the middle dimensions have to pair up, and the resulting matrix has the outside dimensions.
\item Notice that \m{\underset{p\times q}{\mat{B}}  \hspace{2mm} \underset{n\times p}{\mat{A}} } does not exist. 
\item In general, \m{\mat{A}\, \mat{B} \neq \mat{B}\, \mat{A}}. Matrix multiplication \myemph{is not commutative}.

\end{myitemize}

\end{frame}




\begin{frame}[fragile]
\frametitle{Matrix multiplication in R}
\begin{myitemize}
\item R interprets the usual multiplication operator \code{*} as elementwise multiplication.
\item Matrix multiplication is done with \code{\%*\%}.
\item Let's check this by an example.
\begin{columns}[T] 
\begin{column}{0.5\textwidth}
<<>>=
U <- matrix(c(2,1,2,-1),2)
U
V <- matrix(c(3,1,1,2),2)
V
@
\end{column}
\begin{column}{0.4\textwidth}
<<>>=
U * V

U %*% V 

@
\end{column}
\end{columns}

\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Matrix multiplication and linear equations}
\begin{myitemize}
\item 
A linear system of \m{n} equations with \m{p} unknown variables, \m{x_1,\dots,x_p} is
\mydisplaymath{
\left.
\begin{array}{ccccccccc}
\textcolor{red}{a_{11}x_1} &\textcolor{red}{+}& \textcolor{red}{a_{12}\, x_2} &\textcolor{red}{+}& \dots &\textcolor{red}{+}& \textcolor{red}{a_{1p}\, x_p} &=& 
%\textcolor{red}{b_1} 
b_1 \\
a_{21}x_1 &+& a_{22}\, x_2 &+& \dots &+& a_{2p}\, x_p &=& b_2 \\
\vdots    & &           & &       & & \vdots    & & \vdots \\
a_{n1}x_1 &+& a_{n2}\, x_2 &+& \dots &+& a_{np}\, x_p &=& b_n 
\end{array}
\hspace{12mm}
\right\} \hspace{2mm}   \Li
}
\item The rule for matrix multiplication lets us write this as \m{\mat{A}\vec{x}=\vec{b}}.
\mydisplaymath{
\left[
\begin{array}{cccc}
\textcolor{red}{a_{11}} & \textcolor{red}{a_{12}} & \dots & \textcolor{red}{a_{1p}} \\
a_{21} & a_{22} & \dots & a_{2p} \\
\vdots &        &       & \vdots  \\
a_{n1} & a_{n2} & \dots & a_{np}
\end{array}
\right]
\left[
\textcolor{red}{
\begin{array}{c}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{array}
}
\right]
=
\left[
\begin{array}{c}
b_1
 \\ b_2 \\ \vdots \\ b_n
\end{array}
\right]
}


\item In red: the first entry in the matrix product \m{\mat{A}\vec{x}} comes from sliding the first row of \m{\mat{A}} down the column of values in \m{\vec{x}}, multiplying each pair of numbers, and adding up these products. This matches the left hand side of the first line of \m{\Li}.

%\item Matrices are good for using computers to solve systems of linear equations.

\end{myitemize}

\end{frame}


%\end{document}


%\end{document}


\begin{frame}[fragile]
\frametitle{Using matrices to solve a system of linear equations}
\begin{myitemize}
\item
We've seen how matrices can represent a system of linear equations as \m{\mat{A}\vec{x}=\vec{b}}.
\item
For a basic linear algebra equation \m{ax=b} we would divide through by \m{a}, or equivalently multiply through by \m{a^{-1}}, to find \m{x=a^{-1}b} when \m{a\neq 0}.
\item Is there a \myemph{matrix inverse} \m{\mat{A}^{-1}} such that \m{\vec{x}=\mat{A}^{-1}\vec{b}} solves the system of linear equations \m{\mat{A}\vec{x}=\vec{b}}?
\item We will see that there is an inverse \m{\mat{A}^{-1}} when the system of linear equations has a unique solution. Since software can compute this inverse, we can solve systems of linear equations easily. 
This is useful in statistics for fitting linear models to datasets.
Understanding when this inverse exists, and what to do when it doesn't, will help us develop appropriate models for data analysis.
\item From the previous slide, we can only expect \m{\mat{A}^{-1}} to exist when \m{p=n}, in which case \m{\mat{A}} is called a \myemph{square matrix}.
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{A matrix product example}

\myquestion.
Let \m{\mat{U}=\left[\begin{array}{cc} 2& 2 \\ 1 & -1 \end{array}\right]} and
\m{\mat{V}=\left[\begin{array}{cc} 3& 1 \\ 1 & 2 \end{array}\right]}. Calculate \m{\mat{U}\mat{V}}.

\vspace{30mm}

We can check our working in R. 
\begin{columns}[T] 
\begin{column}{0.3\textwidth}
<<>>=
U <- matrix(
   c(2,1,2,-1),2)
U
@
\end{column}
\begin{column}{0.3\textwidth}
<<>>=
V <- matrix(
   c(3,1,1,2),2)
V
@
\end{column}
\begin{column}{0.3\textwidth}
<<>>=
U %*% V 

@
\end{column}
\end{columns}
\end{frame}

\begin{frame}[fragile]
\frametitle{Addition of matrices and multiplication by a scalar}
\begin{myitemize}
\item If \m{\mat{A}=[a_{ij}]_{p\times q}} and \m{\mat{B}=[b_{ij}]_{p\times q}} then the \myemph{matrix sum} \m{\mat{A}+\mat{B}} is computed elementwise, just like for vectors:
{\small
\mydisplaymath{
\mysmallmatrix{a}{p}{q}+\mysmallmatrix{b}{p}{q} =
\left[
\begin{array}{ccc}
a_{11}+b_{11} & \dots & a_{1q}+b_{1q} \\
\vdots & \ddots & \vdots \\
a_{p1}+b_{p1} & \dots & a_{pq}+b_{pq} 
\end{array}
\right]
}
}
\item \myemph{Scalar times matrix} multiplication is also computed elementwise:
{\small
\mydisplaymath{
s\mat{A}=s
\mysmallmatrix{a}{p}{q} =
\left[
\begin{array}{ccc}
s\, a_{11} & \dots & s \, a_{1q} \\
\vdots & \ddots & \vdots \\
s\, a_{p1} & \dots & s\, a_{pq} 
\end{array}
\right]
}
}
\item Scalar times matrix multiplication does commute: \m{s\mat{A} = \mat{A}s}.
\item Matrix and scalar multiplication both have a \myemph{distributive} property: 
\m{\mat{U}(\mat{V}+\mat{W})=\mat{U}\mat{V} + \mat{U}\mat{W}}, and
\m{s(\mat{V}+\mat{W})=s\mat{V} + s\mat{W}}, 
\end{myitemize}

\end{frame}


\begin{frame}[fragile]
\frametitle{The identity matrix}

\vspace{-2mm}

\begin{myitemize}
\item The \m{p\times p} \myemph{identity matrix}, \m{\mat{I}_p}, is a square matrix with 1's on the diagonal and 0's everywhere else: 
\mydisplaymath{
  \mat{I}_p =  \left[
\begin{array}{ccccc}
1 & 0 & \dots & 0 \\
0 & 1 & \dots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 &  \dots & 1 
\end{array}
\right]
}
\item Check that for any \m{p\times p} matrix \m{\mat{A}}, we have \m{\mat{I}_p\mat{A}=\mat{A}\mat{I}_p=\mat{A}}. Also, for any vector \m{\vec{v}\in \Rspace^p} we have \m{\mat{I}_p\vec{v}=\vec{v}}.
\item We can often write \m{\mat{I}} in place of \m{\mat{I}_p} since the dimension of \m{\mat{I}} is always evident from the context. 
\end{myitemize}

\vspace{-2mm}

\myquestion. 
Suppose \m{\mat{B}} is a \m{n\times q} matrix and \m{\mat{I}} is an identity matrix.

\vspace{-2mm}

(i) If we write \m{\mat{B}\mat{I}}, what must be the dimension of \m{\mat{I}}? Find a simplification of \m{\mat{B}\mat{I}}.\\

\vspace{5mm}

(ii) How about if we write \m{\mat{I}\mat{B}}?

\vspace{5mm}
\end{frame}

%\end{document}

\begin{frame}[fragile]
\frametitle{Inverting a $2\times 2$ matrix}
\begin{myitemize}
\item Let \m{\displaystyle \mat{A}=\mytwomatrix{a}{b}{c}{d}} be a general \m{2\times 2} matrix.
\item Let \m{\displaystyle \mat{A}^{-1} = \frac{1}{ad-bc}\mytwomatrix{d}{-b}{-c}{a} }
\end{myitemize}
\myquestion. Compute the matrix product \m{\mat{A}\,\mat{A}^{-1}}

\vspace{25mm}

\begin{myitemize}
\item \m{\mat{A}^{-1}} is called the \myemph{inverse} of \m{\mat{A}}.


\end{myitemize}

\end{frame}

%\end{document}

\begin{frame} 
\frametitle{The identity matrix}
\begin{myitemize}
\item \m{\mat{I}=\mytwomatrix{1}{0}{0}{1}} is the \m{2\times 2} \myemph{identity matrix}.
\item We can check that \m{\mat{A}\, \mat{A}^{-1}= \mat{A}^{-1}\, \mat{A}=\mat{I}}.
\item Also, for any \m{2\times 2} matrix \m{\mat{B}},  \m{\mat{I}\,\mat{B}=\mat{B}\mat{I}=\mat{B}}.
\item We see that \m{\mat{I}} plays the role of 1 in the usual arithmetic. 
\item For numbers we are familiar with the corresponding results 
\m{a^{-1}\dot a = a \dot a^{-1} = 1} and \m{1\times a = a\times 1 = a}.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The determinant of a $2\times 2$ matrix}
\begin{myitemize}
\item Recall that \m{ \mat{A}^{-1}={\displaystyle \mytwomatrix{a}{b}{c}{d}}^{-1} = \frac{1}{ad-bc}\mytwomatrix{d}{-b}{-c}{a} }
\item We call \m{ad-bc} the \myemph{determinant} of \m{\mat{A}}, and we write
\mydisplaymath{\mathrm{det}(A) = ad-bc.}
\item We can see from the formula for \m{ \mat{A}^{-1}} that the inverse of \m{\mat{A}} exists if and only if \m{ad-bc \neq 0}.
\end{myitemize}
%\myquestion. 
%What does it mean geometrically for \m{ad-bc = 0}?

%\vspace{20mm}

%Hint: the slope of the line \m{ax+by=u} is \m{-a/b}, and the slope of \m{cx+dy=v} is \m{-c/d}. 

\end{frame}

<<,echo=F>>=
set.seed(1)
@

%\end{document}

\begin{frame}[fragile]
\frametitle{Finding the matrix inverse and determinant in R}

\vspace{-2mm}

\begin{myitemize}
\item The R function \code{det()} finds the determinant of a square matrix, and \code{solve()} finds
 the inverse if it exists. 
\end{myitemize}

\vspace{-2mm}

\begin{columns}[T] 
\begin{column}{0.5\textwidth}
<<>>=
A <- matrix(runif(9),3,3)
round(A,2)
@
\end{column}
\begin{column}{0.4\textwidth}
<<>>=
A_inv <- solve(A) 
round(A_inv,2)
@
\end{column}
\end{columns}

\vspace{5mm}

\begin{columns}[T] 
\begin{column}{0.5\textwidth}
<<>>= 
A %*% A_inv
@
\end{column}
\begin{column}{0.4\textwidth}
<<>>= 
det(A) ; det(A_inv) 
@
\end{column}
\end{columns}

\vspace{3mm}

\myquestion. 
Why is \url{A %*% A_inv} not exactly equal to the identity matrix?
 
\vspace{5mm}

\end{frame}


\begin{frame}[fragile]
\frametitle{Using R to solve a set of linear equations}

{\bf Worked example}.
Suppose we want to solve
\mydisplaymath{\begin{array}{ccccccccc}
w &+& 2x &-& 3y &+& 4z &=& 0 \\
2w &-& 2x &+& y &+& z &=& 1 \\
-w &-& x &+& 4y &-& z &=& 2 \\
3w &-& x &-& 8y &+& 2z &=& 3 
\end{array}}
How do we do this using R?

1. Write the system as a matrix equation \m{\mat{A}\vec{x}=\vec{b}},

\mydisplaymath{
\left[
\begin{array}{cccc}
1 & 2 &- 3 & 4 \\
2 &- 2 &1 &1  \\
-1 &-1 & 4 &-1 \\
3 &-1  &-8 & 2 
\end{array}
\right]
\left[
\begin{array}{c}
w \\
x \\
y \\
z
\end{array}
\right]
=
\left[
\begin{array}{c}
0 \\
1 \\
2 \\
3
\end{array}
\right]
}

\end{frame}


\begin{frame}[fragile]
\frametitle{Using R to solve a set of linear equations, continued...}
2. Enter the matrix \m{\mat{A}} and vector \m{\vec{b}} into R.
\begin{columns}[T] 
\begin{column}{0.5\textwidth}
<<>>=
A <- rbind( c( 1, 2,-3, 4),
            c( 2,-2, 1, 1),
            c(-1,-1, 4,-1),
            c( 3,-1,-8, 2))
b <- c(0,1,2,3)
@
\end{column}
\begin{column}{0.5\textwidth}

\end{column}
\end{columns}

\vspace{2mm}

3. Compute the matrix solution to the linear system, \m{\vec{x}=\mat{A}^{-1}\vec{b}}.


\vspace{1mm}

\myquestion.
Which of these correctly computes \m{\vec{x}} and why?
\begin{columns}[T] 
\begin{column}{0.4\textwidth}
<<>>=
round(solve(A) %*% b,2)
@
\end{column}
\begin{column}{0.5\textwidth}
<<>>=
round(solve(A) * b,2)
@
\end{column}
\end{columns}

\end{frame}


\begin{frame}[fragile]
\frametitle{The transpose of a matrix}
\begin{myitemize}
\item Sometimes we want to switch the rows and columns of a matrix. 
\item For example, we usually suppose that each column of a data matrix is a measurement variable (say, height and weight) and each row of a data matrix is an object being measured (say, a row for each person). However, what if the data were stored in a matrix where columns corresponded to objects?
\item Switching rows and columns is called \myemph{transposing} the matrix. 
\item The \myemph{transpose} of \m{\mat{A}} is denoted mathematically by \m{\mat{A}^\transpose} and in R by \code{t(A)}.
\end{myitemize}
\mydisplaymath{
\mat{A}=\left[
\begin{array}{ccc}
1 & 2 &- 3 \\
2 &- 2 &1  \\
-1 &-1 & 4 \\
3 &-1  &-8  
\end{array}
\right]
, \quad \quad
\mat{A}^\transpose=
\left[
\begin{array}{cccc}
1 & 2 & -1 & 3 \\
2 & -2 & -1 & -1 \\
-3 & 1 & 4 & -8
\end{array}
\right]
}
\begin{myitemize}
\item
If \m{\mat{A}} has dimension \m{n\times p}, then \m{\mat{A}^\transpose} is \m{p\times n}.
\end{myitemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{More properties of matrices}
\begin{myitemize}
\item The following material is not essential for this course.
However, it may help reinforce understanding to see more ways in which matrix addition and multiplication behave similarly, or differently, from usual arithmetic.
\end{myitemize}
\myemph{Associative property}. We are used to the associative property of addition and multiplication for numbers: \m{a+(b+c)=(a+b)+c} and \m{a\times(b\times c) = (a\times b)\times c}. 
You can check that matrix addition and multiplication also have the associative property: for matrices of appropriate size,
\m{\mat{A}+(\mat{B}+\mat{C})=(\mat{A}+\mat{B})+\mat{C}} and \m{\mat{A}(\mat{B}\, \mat{C}) = (\mat{A}\, \mat{B})\mat{C}}.

\myemph{Inverse of a product}. For square invertible matrices \m{\mat{A}} and \m{\mat{B}}, we can check that \m{(\mat{A}\mat{B})^{-1} = \mat{B}^{-1}\mat{A}^{-1}}. The change of order may seem weird.
To demonstrate that this inverse works correctly,
\mydisplaymath{
(\mat{A}\, \mat{B})^{-1}(\mat{A}\, \mat{B}) = \mat{B}^{-1}\mat{A}^{-1}\mat{A}\, \mat{B} = \mat{B}^{-1}\mat{I}\, \mat{B} = \mat{B}^{-1}\mat{B} = \mat{I}.
}
Note that we have repeatedly used the associative property of matrix multiplication, and we have been careful not to accidentally commute (recall that, in general, \m{\mat{C}\, \mat{D} \neq \mat{D}\, \mat{C}}).

\end{frame}


\begin{frame}[fragile]
\frametitle{More properties of matrices, continued}
\myemph{Transpose of a sum}. Convince yourself that \m{(\mat{A}+\mat{B})^\transpose = \mat{A}^\transpose + \mat{B}^\transpose}. If you like, calculate an example in R to check.

\myemph{Transpose of a product}. The rule is
\m{
(\mat{A}\, \mat{B})^\transpose = \mat{B}^\transpose \mat{A}^\transpose
}.

\myquestion. 
Suppose that \m{\mat{A}} has dimension \m{n\times p} and \m{\mat{B}} is \m{p\times q}. Check that this formula for \m{(\mat{A}\, \mat{B})^\transpose} has the right dimension.

\vspace{10mm}

\myexample:
<<>>= 
A <- matrix(1:3,4,3); B <- matrix(1:6,3,2)
@

\vspace{3mm}

\begin{columns}[T] 
\begin{column}{0.45\textwidth}
<<>>= 
t(A %*% B)
@
\end{column}
\begin{column}{0.45\textwidth}
<<>>= 
t(B) %*% t(A)
@
\end{column}
\end{columns}

\end{frame}

\begin{frame}[fragile]
\frametitle{More properties of matrices, continued}

{\bf A matrix commutes with its inverse}: \m{\mat{A}^{-1}\mat{A} = \mat{A}\mat{A}^{-1} = \mat{I}}.

\begin{myitemize}
\item Recall that, in general, matrix multiplication does not commute \m{(\mat{A}\mat{B}\neq \mat{B}\mat{A}}).

\item We can check, using R, that a matrix does commute with its inverse.

\end{myitemize}

<<,echo=F>>=
set.seed(2)
@

\vspace{-2mm}

\begin{columns}[T] 
\begin{column}{0.4\textwidth}
<<>>= 
A <- matrix(runif(9),3,3)
@
\end{column}
\begin{column}{0.4\textwidth}
<<>>= 
A_inv <- solve(A)
@
\end{column}
\end{columns}

\vspace{5mm}

\begin{columns}[T] 
\begin{column}{0.4\textwidth}
<<>>= 
round( A %*% A_inv, 3)
@
\end{column}
\begin{column}{0.4\textwidth}
<<>>= 
round( A_inv %*% A, 3)
@
\end{column}
\end{columns}

\end{frame}

\end{document}

\end{frame}

\begin{frame}[fragile]
\frametitle{}
\end{frame}



%%%%%% KEPT FOR LATER, MAYBE WHEN WE'RE DOING RANK


\begin{frame}[fragile]
\frametitle{Does a system of linear equations have no solution? One solution? Many solutions?}
\begin{myitemize}
\item One linear equation in one unknown, \m{ax=u}, has a unique solution unless \m{a=0}.
\item One linear equation with two unknowns, \m{ax+by=u}, has a solution consisting of all points on a line in the \m{x-y} plane, as long as one of \m{a} and \m{b} is nonzero.
\item Two linear equations with two unknowns, 
\altdisplaymath{
\begin{array}{ccccc}
ax &+& by &=& u \\
cx &+& dy &=& v
\end{array}
},
have a unique solution where the lines \m{ax+by=u} and \m{cx+dy=v} intersect, so long as these lines are not parallel.
\item Three linear equations in two unknowns will not usually have a solution---the three corresponding lines would all have to meet at a common point. 
\item Can you see the general pattern?
\end{myitemize}

\end{frame}

\begin{frame}[fragile]
\frametitle{Does a system of linear equations have no solution? One solution? Many solutions? Continued...}
\begin{myitemize}
\item For three unknowns, an equation \m{ax+by+cz=u} corresponds to a plane in three-dimensional \m{(x,y,z)} space.
\item Three planes will typically intersect at a single point, so three equations in three unknowns will typically have a unique solution.
\item Two planes that are not parallel will meet along a line, and give a family of solutions.
\item Four or more planes will typically not all meet at any point.
\item In higher dimensions, we can't visualize but the pattern remains true.
\item The general linear system we wrote previously in \myref{\Li} has \m{n} equations with \m{p} unknowns. 
We expect a unique solution when \m{p=n}, no
solution when \m{p<n} and a family of solutions when \m{p>n}.
\end{myitemize}

\end{frame}




\begin{frame}[fragile]
\frametitle{Column vectors and row vectors}
\begin{myitemize}
\item
In the matrix times vector multiplication on the previous slide, the vector \m{\vec{x}} is written in a column, as a \m{p\times 1} matrix.
\item 
We say that that \m{\vec{x}} is a \myemph{column vector}. We interpret a vector \m{\vec{x}} as a column vectors unless we explicitly say it is a \m{1\times p} \myemph{row vector}. 
\item
Similarly, \m{\vec{b}} in the previous slide is a length \m{n} column vector.
\item
R matches our notation: a vector in R is not a matrix, but is interpreted as a column vector for matrix times vector multiplication. R uses \code{\%*\%} to denote matrix multiplication.
\end{myitemize}
\begin{columns}[T] 
\begin{column}{0.22\textwidth}
<<>>=
x <- c(1,2)
is(x,"matrix")
is(x,"vector")
@
\end{column}
\begin{column}{0.3\textwidth}
<<>>=
A<-matrix(
 c(1,0,0,-1),nrow=2)
A %*% x
@
\end{column}
\begin{column}{0.3\textwidth}
<<>>=
xx<-matrix(x,nrow=2)

A %*% xx
@
\end{column}
\end{columns}

 
\end{frame}

